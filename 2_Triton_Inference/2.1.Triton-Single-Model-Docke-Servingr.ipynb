{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [모듈 2.1] Triton Docker 에 한개의 NCF 모델 서빙"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 환경 셋업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1. 기본 세팅\n",
    "사용하는 패키지는 import 시점에 다시 재로딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('./src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전 노트북에서 훈련 후의 아티펙트를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. 배포 준비\n",
    "\n",
    "### 이전 노트북에서 훈련된 모델의 S3 경로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r artifact_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model artifact is assigend from : \", artifact_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추론을 위한  데이터 세트 로딩\n",
    "- 전부 데이터를 로딩할 필요가 없지만, 여기서는 기존에 사용한 함수를 이용하기 위해서 전체 데이터를 로드 합니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_utils \n",
    "train_data, test_data, user_num ,item_num, train_mat = data_utils.load_all(test_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파라미터 생성\n",
    "- 모델 로딩시에 아라 파라미터 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    def __init__(self):\n",
    "        # self.epochs = 1        \n",
    "        self.num_ng = 4\n",
    "        self.batch_size = 256\n",
    "        self.test_num_ng = 99\n",
    "        self.factor_num = 32\n",
    "        self.num_layers = 3\n",
    "        self.dropout = 0.0\n",
    "        # self.lr = 0.001\n",
    "        self.top_k = 10\n",
    "        self.out = True\n",
    "        # self.gpu = \"0\"\n",
    "                        \n",
    "args = Params()\n",
    "print(\"# of batch_size: \", args.batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 훈련된 모델 아티펙트 다운로드 및 압축해제\n",
    "- 모델 아티펙트를 다운로드 합니다.\n",
    "- 다운로드 받은 모델 아티펙트의 압축을 해제하고 모델 가중치인 models/model.pth 파일을 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import config\n",
    "\n",
    "model_data_dir = config.model_path\n",
    "os.makedirs(model_data_dir, exist_ok=True)\n",
    "print(\"model_data_dir: \", model_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh -s {artifact_path} {model_data_dir}\n",
    "\n",
    "artifact_path=$1\n",
    "model_data_dir=$2\n",
    "\n",
    "echo $artifact_path\n",
    "echo $model_data_dir\n",
    "\n",
    "# 기존 데이터 삭제\n",
    "rm -rf $model_data_dir/*\n",
    "\n",
    "# 모델을 S3에서 로컬로 다운로드\n",
    "aws s3 cp $artifact_path $model_data_dir\n",
    "\n",
    "# 모델 다운로드 폴더로 이동\n",
    "cd $model_data_dir\n",
    "\n",
    "# 압축 해제\n",
    "tar -xvf model.tar.gz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 훈련된 모델 로딩\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. 모델 네트워크 설정 저장\n",
    "- 모델 네트워크를 생성시에 사용할 설정값을 model_config.json 로 저장함.\n",
    "- model_fn() 함수에서 모델 네트워크를 생성시에 사용 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from common_utils import save_json, load_json\n",
    "\n",
    "model_config_dict = {\n",
    "    'user_num': str(user_num),\n",
    "    'item_num': str(item_num),\n",
    "    'factor_num' : str(args.factor_num),\n",
    "    'num_layers' : str(args.num_layers),\n",
    "    'dropout' : str(args.dropout),\n",
    "    'model_type': config.model\n",
    "}\n",
    "\n",
    "model_config_file = 'model_config.json'\n",
    "model_config_file_path = os.path.join('src', model_config_file)\n",
    "\n",
    "save_json(model_config_file_path, model_config_dict)\n",
    "# model_config_dict = load_json(model_config_file_path)    \n",
    "# model_config_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.2. 모델 로딩\n",
    "- 복수개의 모델로 진행하기 위해서, 편의상 동일한 모델에서 생성 함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import model_fn\n",
    "\n",
    "ncf_food_model = model_fn(config.model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncf_food_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Trition 서빙 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. 샘플 입력 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "user_np = np.zeros((1,100)).astype(np.int32)\n",
    "item_np = np.random.randint(low=1, high=1000, size=(1,100)).astype(np.int32)\n",
    "\n",
    "dummy_inputs = [\n",
    "    torch.from_numpy(user_np).to(device),\n",
    "    torch.from_numpy(item_np).to(device)\n",
    "]\n",
    "print(\"dummy_inputs: \\n\", dummy_inputs)\n",
    "dummy_user = dummy_inputs[0] \n",
    "dummy_item = dummy_inputs[1] \n",
    "\n",
    "# dummy_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. 샘플 입력으로 모델 추론 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ncf_food_model(dummy_user, dummy_item)\n",
    "print(\"result shape: \", result.shape)\n",
    "# result = ncf_fashion_model(dummy_user, dummy_item)\n",
    "# print(\"result shape: \", result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Torch Script 으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_trace = True\n",
    "is_script = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_model(mode, device, model, dummy_inputs, trace_model_name):\n",
    "\n",
    "    model = model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    if mode == 'trace' :\n",
    "        IR_model = torch.jit.trace(model, dummy_inputs)\n",
    "\n",
    "    elif mode == 'script':\n",
    "        IR_model = torch.jit.script(model)\n",
    "\n",
    "    print(f\"As {mode} : Model is saved {trace_model_name}\")\n",
    "    torch.jit.save(IR_model, trace_model_name)\n",
    "\n",
    "    print(\"#### Load Test ####\")    \n",
    "    loaded_m = torch.jit.load(trace_model_name)    \n",
    "    print(loaded_m.code)    \n",
    "    dummy_user = dummy_inputs[0]\n",
    "    dummy_item = dummy_inputs[1]    \n",
    "    \n",
    "    result = loaded_m(dummy_user, dummy_item)\n",
    "    print(\"Result shape: \", result.shape)\n",
    "\n",
    "        \n",
    "if is_trace:\n",
    "    mode = 'trace'    \n",
    "elif is_script:    \n",
    "    mode = 'script'\n",
    "\n",
    "# food\n",
    "trace_food_model_name = 'ncf_food_model.pt'    \n",
    "trace_model(mode, device, ncf_food_model, dummy_inputs, trace_food_model_name)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.config.pbtxt 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ncf_food_config 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ncf_food_config.pbtxt\n",
    "name: \"ncf_food_model\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 128\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [100]\n",
    "  },\n",
    "  {\n",
    "    name: \"INPUT__1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [100]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [-1]\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 아티펙트 패키징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 싱글 모델\n",
    "- 아래와 닽은 폴더 구조를 생성해야 함.\n",
    "```\n",
    "model_serving_folder\n",
    "    - model_name\n",
    "        - version_number\n",
    "            - model file\n",
    "        - config file\n",
    "        \n",
    "# Example: \n",
    "\n",
    "triton-serve-pt\n",
    "    - ncf_food\n",
    "        - 1\n",
    "            - model.pt\n",
    "        - config.pbtxt\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. ncf_food_model 폴더 생성 및 아티펙트 카피"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from triton_util import make_folder_structure, copy_artifact, remove_folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncf_food_model 폴더 생성\n",
    "model_serving_folder = 'triton-docker-serve-pt'\n",
    "model_name = 'ncf_food_model'\n",
    "make_folder_structure(model_serving_folder, model_name)\n",
    "\n",
    "fodd_config = 'ncf_food_config.pbtxt'\n",
    "copy_artifact(model_serving_folder, model_name, trace_food_model_name, fodd_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 폴더 삭제\n",
    "- 필요시 주석 제거하고 사용하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_serving_folder = 'triton-docker-serve-pt'\n",
    "# remove_folder(model_serving_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. 로컬 도커에서 실행 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0. 도커에서의 실행 테스트는 아래와 같은 순서로 진행 함.\n",
    "\n",
    "#### (0) Triton Client 초기화\n",
    "```\n",
    "from triton_util import setup_triton_client\n",
    "triton_client, grpcclient = setup_triton_client()\n",
    "```\n",
    "\n",
    "#### (1) 터미널 실행\n",
    "![terminal.png](img/terminal.png)\n",
    "\n",
    "#### (2) Triton 도커 컨테이너 실행\n",
    "- 위의 터미널에 아래와 같이 명령어를 하나씩 실행 하세요.\n",
    "```\n",
    "cd /home/ec2-user/SageMaker/Neural-Collaborative-Filtering-On-SageMaker/2_Triton_Inference\n",
    "\n",
    "docker run --gpus=1 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v `pwd`/triton-docker-serve-pt:/models nvcr.io/nvidia/tritonserver:22.08-py3 tritonserver --model-repository=/models --log-verbose=3 --log-info=1 --log-warning=1 --log-error=1\n",
    "```\n",
    "#### (3) Triton 클라이언트로 추론 실행\n",
    "#### (4) 도커 중단 및 삭제\n",
    "```\n",
    "docker rm -f $(docker ps -qa)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Triton Client 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from triton_util import setup_triton_client\n",
    "triton_client, grpcclient = setup_triton_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. !!! #### 터미널에 \"Triton 도커 컨테이너 실행\" 을 해주세요. ### !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. 입력 payload 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_client_payload():\n",
    "    inputs = []\n",
    "\n",
    "    inputs.append(grpcclient.InferInput('INPUT__0', [1,100], \"INT32\"))\n",
    "    inputs.append(grpcclient.InferInput('INPUT__1', [1,100], \"INT32\"))\n",
    "\n",
    "    # user\n",
    "    input0_data = np.zeros((1,100)).astype(np.int32)\n",
    "    inputs[0].set_data_from_numpy(input0_data)\n",
    "\n",
    "    # item\n",
    "    input1_data = np.random.randint(low=1, high=1000, size=(1,100)).astype(np.int32)\n",
    "    inputs[1].set_data_from_numpy(input1_data)\n",
    "\n",
    "    print(\"input0_data: \\n\",input0_data) \n",
    "    print(\"input1_data: \\n\",input1_data) \n",
    "    \n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = create_client_payload()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. 출력 변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "outputs.append(grpcclient.InferRequestedOutput('OUTPUT__0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6.5. Triton에 추론 요청\n",
    "- 추론 요청이 오면 서버에서 처맇하여 결과를 내보냄. 서버에서의 처리 내용 로그를 한번 보세요.\n",
    "![single_triton_server_log.png](img/single_triton_server_log.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from triton_util import infer_triton_client\n",
    "\n",
    "model_name = \"ncf_food_model\"\n",
    "infer_triton_client(triton_client, model_name, inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 변수 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store model_serving_folder\n",
    "%store model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
