{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [모듈 1.0] 워밍업 : Triton Docker 처음 시작하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. 환경 셋업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1. 기본 세팅\n",
    "사용하는 패키지는 import 시점에 다시 재로딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('./src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hello Model 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class MyCell(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCell, self).__init__()\n",
    "        self.linear = torch.nn.Linear(4, 4)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        new_h = torch.tanh(self.linear(x) + h)\n",
    "        return new_h\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: \n",
      "tensor([[0.9908, 0.9052, 0.8996, 0.0180],\n",
      "        [0.1980, 0.2825, 0.8672, 0.2721],\n",
      "        [0.7820, 0.6974, 0.6783, 0.0481]]), \n",
      " h: \n",
      "tensor([[0.7485, 0.3368, 0.1435, 0.2451],\n",
      "        [0.9100, 0.6574, 0.9836, 0.2775],\n",
      "        [0.4921, 0.1276, 0.6965, 0.3775]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.8283, 0.5090, 0.5656, 0.1928],\n",
       "        [0.8522, 0.4230, 0.7423, 0.0349],\n",
       "        [0.7493, 0.3181, 0.7490, 0.2462]], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_cell = MyCell()\n",
    "x, h = torch.rand(3, 4), torch.rand(3, 4)\n",
    "\n",
    "print(f\"x: \\n{x}, \\n h: \\n{h}\")\n",
    "my_cell(x,h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Trition 서빙 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Torch Script 으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "def trace_model(mode, device, model, dummy_inputs, trace_model_name):\n",
    "\n",
    "    model = model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    if mode == 'trace' :\n",
    "        IR_model = torch.jit.trace(model, dummy_inputs)\n",
    "\n",
    "    elif mode == 'script':\n",
    "        IR_model = torch.jit.script(model)\n",
    "\n",
    "    print(f\"As {mode} : Model is saved {trace_model_name}\")\n",
    "    torch.jit.save(IR_model, trace_model_name)\n",
    "\n",
    "    print(\"#### Load Test ####\")    \n",
    "    loaded_m = torch.jit.load(trace_model_name)    \n",
    "    print(loaded_m.code)    \n",
    "    dummy_user = dummy_inputs[0]\n",
    "    dummy_item = dummy_inputs[1]    \n",
    "    \n",
    "    result = loaded_m(dummy_user, dummy_item)\n",
    "    print(\"Result : \\n\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy_inputs: \n",
      " [tensor([[0.9908, 0.9052, 0.8996, 0.0180],\n",
      "        [0.1980, 0.2825, 0.8672, 0.2721],\n",
      "        [0.7820, 0.6974, 0.6783, 0.0481]], device='cuda:0'), tensor([[0.7485, 0.3368, 0.1435, 0.2451],\n",
      "        [0.9100, 0.6574, 0.9836, 0.2775],\n",
      "        [0.4921, 0.1276, 0.6965, 0.3775]], device='cuda:0')]\n",
      "As script : Model is saved hello_model.pt\n",
      "#### Load Test ####\n",
      "def forward(self,\n",
      "    x: Tensor,\n",
      "    h: Tensor) -> Tensor:\n",
      "  _0 = torch.add((self.linear).forward(x, ), h, alpha=1)\n",
      "  return torch.tanh(_0)\n",
      "\n",
      "Result : \n",
      " tensor([[0.8283, 0.5090, 0.5656, 0.1928],\n",
      "        [0.8522, 0.4230, 0.7423, 0.0349],\n",
      "        [0.7493, 0.3181, 0.7490, 0.2462]], device='cuda:0',\n",
      "       grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "is_trace = False\n",
    "is_script = True\n",
    "        \n",
    "if is_trace:\n",
    "    mode = 'trace'    \n",
    "elif is_script:    \n",
    "    mode = 'script'\n",
    "\n",
    "dummy_inputs = [\n",
    "    x.to(device),h.to(device)\n",
    "    ]\n",
    "print(\"dummy_inputs: \\n\", dummy_inputs)\n",
    "        \n",
    "trace_model_name = 'hello_model.pt'    \n",
    "trace_model(mode, device, my_cell, dummy_inputs, trace_model_name)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.config.pbtxt 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hello_config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile hello_config.pbtxt\n",
    "name: \"hello\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 128\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [3,4]\n",
    "  },\n",
    "  {\n",
    "    name: \"INPUT__1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [3,4]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [128,3,4]\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. 아티펙트 패키징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 리파지토리 폴더 구조\n",
    "```\n",
    "model_serving_folder\n",
    "    - model_name\n",
    "        - version_number\n",
    "            - model file\n",
    "        - config file\n",
    "\n",
    "# Example\n",
    "hello-serve-pt\n",
    "    - hello\n",
    "        - 1\n",
    "            - model.pt\n",
    "        - config.pbtxt\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. hello 폴더 생성 및 아티펙트 카피"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triton-hello-serve-pt:\n",
      "hello\n",
      "\n",
      "triton-hello-serve-pt/hello:\n",
      "1\n",
      "config.pbtxt\n",
      "\n",
      "triton-hello-serve-pt/hello/1:\n",
      "model.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from triton_util import make_folder_structure, copy_artifact, remove_folder\n",
    "\n",
    "# triton-hello-serve-pt 폴더 생성\n",
    "model_serving_folder = 'triton-hello-serve-pt'\n",
    "model_name = 'hello'\n",
    "make_folder_structure(model_serving_folder, model_name)\n",
    "\n",
    "model_config = 'hello_config.pbtxt'\n",
    "copy_artifact(model_serving_folder, model_name, trace_model_name, model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 폴더 삭제\n",
    "- 필요시 주석 제거하고 사용하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triton-hello-serve-pt is removed\n"
     ]
    }
   ],
   "source": [
    "# model_serving_folder = 'triton-hello-serve-pt'\n",
    "# remove_folder(model_serving_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5. 로컬 도커에서 실행 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0. 도커에서의 실행 테스트는 아래와 같은 순서로 진행 함.\n",
    "\n",
    "#### (0) Triton Client 초기화\n",
    "```\n",
    "from triton_util import setup_triton_client\n",
    "triton_client, grpcclient = setup_triton_client()\n",
    "```\n",
    "\n",
    "#### (1) 터미널 실행\n",
    "![terminal.png](img/terminal.png)\n",
    "\n",
    "#### (2) Triton 도커 컨테이너 실행\n",
    "- 위의 터미널에 아래와 같이 명령어를 하나씩 실행 하세요.\n",
    "```\n",
    "cd /home/ec2-user/SageMaker/Neural-Collaborative-Filtering-On-SageMaker/2_Triton_Inference\n",
    "\n",
    "docker run --gpus=1 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v `pwd`/triton-hello-serve-pt:/models nvcr.io/nvidia/tritonserver:22.08-py3 tritonserver --model-repository=/models --log-verbose=3 --log-info=1 --log-warning=1 --log-error=1\n",
    "```\n",
    "#### (3) Triton 클라이언트로 추론 실행\n",
    "#### (4) 도커 중단 및 삭제\n",
    "```\n",
    "docker rm -f $(docker ps -qa)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Triton Client 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from triton_util import setup_triton_client\n",
    "triton_client, grpcclient = setup_triton_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. !!! #### 터미널에 \"Triton 도커 컨테이너 실행\" 을 해주세요. ### !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. 입력 payload 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_client_payload():\n",
    "    inputs = []\n",
    "\n",
    "    inputs.append(grpcclient.InferInput('INPUT__0', [1,3,4], \"FP32\"))\n",
    "    inputs.append(grpcclient.InferInput('INPUT__1', [1,3,4], \"FP32\"))\n",
    "\n",
    "    input0_data = np.random.randn(1,3,4).astype(np.float32)\n",
    "    # Initialize the data\n",
    "    inputs[0].set_data_from_numpy(input0_data)\n",
    "\n",
    "\n",
    "    input1_data = np.random.randn(1,3,4).astype(np.float32)\n",
    "    inputs[1].set_data_from_numpy(input0_data)\n",
    "\n",
    "    print(\"input0_data: \\n\",input0_data) \n",
    "    print(\"input1_data: \\n\",input1_data) \n",
    "    \n",
    "    return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input0_data: \n",
      " [[[-1.7612643  -0.7525186  -0.98555493  0.8345339 ]\n",
      "  [ 1.231041    0.4869961   0.5787533   1.8141615 ]\n",
      "  [ 0.24886777 -1.0502387   1.3411019   0.88277024]]]\n",
      "input1_data: \n",
      " [[[-1.0084157  -0.24527276  0.6400702   1.1905762 ]\n",
      "  [-0.44431686  0.41970655  0.5309932   0.11136666]\n",
      "  [-0.1452226   2.5263047   1.3037627   0.58281195]]]\n"
     ]
    }
   ],
   "source": [
    "inputs = create_client_payload()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. 출력 변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "outputs.append(grpcclient.InferRequestedOutput('OUTPUT__0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5.5. Triton에 추론 요청\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### output #####\n",
      "(1, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "from triton_util import infer_triton_client\n",
    "\n",
    "infer_triton_client(triton_client, model_name, inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
