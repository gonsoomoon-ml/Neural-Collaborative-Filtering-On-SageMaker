{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [모듈 2.2] SageMaker 앤드포인트에 한개의 모델 Triton 배포"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 환경 셋업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1. 기본 세팅\n",
    "사용하는 패키지는 import 시점에 다시 재로딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('./src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전 노트북에서 훈련 후의 아티펙트를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r model_serving_folder\n",
    "%store -r model_name\n",
    "%store -r bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "import boto3, json, sagemaker, time\n",
    "import numpy as np\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. 변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"triton-ncf\"\n",
    "\n",
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "# endpoint variables\n",
    "sm_model_name = f\"{prefix}-mdl-{ts}\"\n",
    "endpoint_config_name = f\"{prefix}-epc-{ts}\"\n",
    "endpoint_name = f\"{prefix}-ep-{ts}\"\n",
    "model_data_url = f\"s3://{bucket}/{prefix}/\"\n",
    "instance_type = \"local_gpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sm_model_name: \n",
      " triton-ncf-mdl-2022-12-11-07-57-19\n",
      "endpoint_config_name: \n",
      " triton-ncf-epc-2022-12-11-07-57-19\n",
      "endpoint_name: \n",
      " triton-ncf-ep-2022-12-11-07-57-19\n"
     ]
    }
   ],
   "source": [
    "print(\"sm_model_name: \\n\", sm_model_name)\n",
    "print(\"endpoint_config_name: \\n\", endpoint_config_name)\n",
    "print(\"endpoint_name: \\n\", endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Triton Docker Image 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mme_triton_image_uri: \n",
      " 785573368785.dkr.ecr.us-east-1.amazonaws.com/sagemaker-tritonserver:22.07-py3\n"
     ]
    }
   ],
   "source": [
    "from triton_util import account_id_map\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "mme_triton_image_uri = (\n",
    "    \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.07-py3\".format(\n",
    "        account_id=account_id_map[region], region=region, base=base\n",
    "    )\n",
    ")\n",
    "print(\"mme_triton_image_uri: \\n\", mme_triton_image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'triton-docker-serve-pt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_serving_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 모델 패키징 (model.tar.gz) 및 S3 업로딩\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxrwxr-x ec2-user/ec2-user 0 2022-12-11 06:59 ncf_food_model/\n",
      "-rw-rw-r-- ec2-user/ec2-user 306 2022-12-11 07:52 ncf_food_model/config.pbtxt\n",
      "drwxrwxr-x ec2-user/ec2-user   0 2022-12-11 06:59 ncf_food_model/1/\n",
      "-rw-rw-r-- ec2-user/ec2-user 6440569 2022-12-11 07:52 ncf_food_model/1/model.pt\n",
      "model_tar_file:  ncf_food_model.model.tar.gz\n",
      "model_uri_pt:  s3://sagemaker-us-east-1-057716757052/triton-ncf/ncf_food_model.model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from triton_util import tar_artifact, upload_tar_s3\n",
    "\n",
    "    \n",
    "model_tar_file = tar_artifact(model_serving_folder, model_name)    \n",
    "print(\"model_tar_file: \", model_tar_file)\n",
    "model_uri_pt = upload_tar_s3(sagemaker_session, model_tar_file, prefix)\n",
    "print(\"model_uri_pt: \", model_uri_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 로컬 모드 설정\n",
    "- 내부적으로 Triton 서버가 구동시에 아래 URL 스크립트가 구동 됨.\n",
    "    - 여기에 맞는 필요한 환경 변수를 넣어 줌.\n",
    "    - https://raw.githubusercontent.com/triton-inference-server/server/main/docker/sagemaker/serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_envs = {\n",
    "                    \"SAGEMAKER_TRITON_LOG_VERBOSE\": \"3\",\n",
    "                    \"SAGEMAKER_TRITON_LOG_INFO\": \"1\",\n",
    "                    \"SAGEMAKER_TRITON_LOG_WARNING\" : \"1\",\n",
    "                    \"SAGEMAKER_TRITON_LOG_ERROR\" : \"1\"\n",
    "                 }\n",
    "\n",
    "local_pytorch_model = Model(model_data= model_uri_pt,\n",
    "                            image_uri = mme_triton_image_uri,\n",
    "                            role=role,\n",
    "                            env = container_envs\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Attaching to holdgwluib-algo-1-5toty\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m \n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m =============================\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m == Triton Inference Server ==\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m =============================\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m \n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m NVIDIA Release 22.07 (build <unknown>)\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m Triton Server Version 2.24.0\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m \n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m Copyright (c) 2018-2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m \n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m \n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m By pulling and using the container, you accept the terms and conditions of this license:\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m \n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m NOTE: CUDA Forward Compatibility mode ENABLED.\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m   Using CUDA 11.7 driver version 515.48.08 with kernel driver version 510.47.03.\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m   See https://docs.nvidia.com/deploy/cuda-compatibility/ for details.\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m \n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m WARNING: No SAGEMAKER_TRITON_DEFAULT_MODEL_NAME provided.\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m          Starting with the only existing model directory ncf_food_model\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:36.479096 91 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7f5de8000000' with size 268435456\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:36.482284 91 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:36.482309 91 cuda_memory_manager.cc:105] CUDA memory pool is created on device 1 with size 67108864\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:36.482316 91 cuda_memory_manager.cc:105] CUDA memory pool is created on device 2 with size 67108864\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:36.482330 91 cuda_memory_manager.cc:105] CUDA memory pool is created on device 3 with size 67108864\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:37.142279 91 model_config_utils.cc:645] Server side auto-completed config: name: \"ncf_food_model\"\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m platform: \"pytorch_libtorch\"\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m max_batch_size: 128\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m input {\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m   name: \"INPUT__0\"\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m   data_type: TYPE_INT32\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m   dims: 100\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m }\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m input {\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m   name: \"INPUT__1\"\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m   data_type: TYPE_INT32\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m   dims: 100\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m }\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m output {\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m   name: \"OUTPUT__0\"\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m   data_type: TYPE_FP32\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m   dims: -1\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m }\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m default_model_filename: \"model.pt\"\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m backend: \"pytorch\"\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m \n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:37.144386 91 model_repository_manager.cc:913] AsyncLoad() 'ncf_food_model'\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:37.144448 91 model_repository_manager.cc:1151] TriggerNextAction() 'ncf_food_model' version 1: 1\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:37.144507 91 model_repository_manager.cc:1187] Load() 'ncf_food_model' version 1\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:37.144514 91 model_repository_manager.cc:1206] loading: ncf_food_model:1\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:37.144589 91 model_repository_manager.cc:1256] CreateModel() 'ncf_food_model' version 1\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:37.145076 91 backend_model.cc:292] Adding default backend config setting: default-max-batch-size,4\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:37.145116 91 shared_library.cc:108] OpenLibraryHandle: /opt/tritonserver/backends/pytorch/libtriton_pytorch.so\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.900928 91 libtorch.cc:1917] TRITONBACKEND_Initialize: pytorch\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.900963 91 libtorch.cc:1927] Triton TRITONBACKEND API version: 1.10\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.901889 91 libtorch.cc:1933] 'pytorch' TRITONBACKEND API version: 1.10\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.901928 91 libtorch.cc:1966] TRITONBACKEND_ModelInitialize: ncf_food_model (version 1)\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.904732 91 model_config_utils.cc:1656] ModelConfig 64-bit fields:\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.904759 91 model_config_utils.cc:1658] \tModelConfig::dynamic_batching::default_queue_policy::default_timeout_microseconds\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.904774 91 model_config_utils.cc:1658] \tModelConfig::dynamic_batching::max_queue_delay_microseconds\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.904782 91 model_config_utils.cc:1658] \tModelConfig::dynamic_batching::priority_queue_policy::value::default_timeout_microseconds\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.904790 91 model_config_utils.cc:1658] \tModelConfig::ensemble_scheduling::step::model_version\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.904797 91 model_config_utils.cc:1658] \tModelConfig::input::dims\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.904803 91 model_config_utils.cc:1658] \tModelConfig::input::reshape::shape\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.904810 91 model_config_utils.cc:1658] \tModelConfig::instance_group::secondary_devices::device_id\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.904822 91 model_config_utils.cc:1658] \tModelConfig::model_warmup::inputs::value::dims\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.904830 91 model_config_utils.cc:1658] \tModelConfig::optimization::cuda::graph_spec::graph_lower_bound::input::value::dim\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.904836 91 model_config_utils.cc:1658] \tModelConfig::optimization::cuda::graph_spec::input::value::dim\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.904843 91 model_config_utils.cc:1658] \tModelConfig::output::dims\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.904857 91 model_config_utils.cc:1658] \tModelConfig::output::reshape::shape\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.904864 91 model_config_utils.cc:1658] \tModelConfig::sequence_batching::direct::max_queue_delay_microseconds\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.904870 91 model_config_utils.cc:1658] \tModelConfig::sequence_batching::max_sequence_idle_microseconds\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.904877 91 model_config_utils.cc:1658] \tModelConfig::sequence_batching::oldest::max_queue_delay_microseconds\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.904884 91 model_config_utils.cc:1658] \tModelConfig::sequence_batching::state::dims\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.904900 91 model_config_utils.cc:1658] \tModelConfig::sequence_batching::state::initial_state::dims\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.904906 91 model_config_utils.cc:1658] \tModelConfig::version_policy::specific::versions\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m W1211 07:57:38.905699 91 libtorch.cc:262] skipping model configuration auto-complete for 'ncf_food_model': not supported for pytorch backend\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.907238 91 libtorch.cc:291] Optimized execution is enabled for model instance 'ncf_food_model'\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.907266 91 libtorch.cc:310] Cache Cleaning is disabled for model instance 'ncf_food_model'\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.907282 91 libtorch.cc:327] Inference Mode is disabled for model instance 'ncf_food_model'\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.907290 91 libtorch.cc:422] NvFuser is not specified for model instance 'ncf_food_model'\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.908349 91 libtorch.cc:2010] TRITONBACKEND_ModelInstanceInitialize: ncf_food_model (GPU device 0)\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:38.911942 91 backend_model_instance.cc:105] Creating instance ncf_food_model on GPU 0 (7.0) using artifact 'model.pt'\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:41.698921 91 backend_model_instance.cc:735] Starting backend thread for ncf_food_model at nice 0 on device 0...\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:41.699109 91 libtorch.cc:2010] TRITONBACKEND_ModelInstanceInitialize: ncf_food_model (GPU device 1)\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:41.699415 91 backend_model_instance.cc:105] Creating instance ncf_food_model on GPU 1 (7.0) using artifact 'model.pt'\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:42.579108 91 backend_model_instance.cc:735] Starting backend thread for ncf_food_model at nice 0 on device 1...\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:42.579251 91 libtorch.cc:2010] TRITONBACKEND_ModelInstanceInitialize: ncf_food_model (GPU device 2)\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:42.579560 91 backend_model_instance.cc:105] Creating instance ncf_food_model on GPU 2 (7.0) using artifact 'model.pt'\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:43.466381 91 backend_model_instance.cc:735] Starting backend thread for ncf_food_model at nice 0 on device 2...\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:43.466510 91 libtorch.cc:2010] TRITONBACKEND_ModelInstanceInitialize: ncf_food_model (GPU device 3)\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:43.466815 91 backend_model_instance.cc:105] Creating instance ncf_food_model on GPU 3 (7.0) using artifact 'model.pt'\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:44.338071 91 backend_model_instance.cc:735] Starting backend thread for ncf_food_model at nice 0 on device 3...\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:44.338291 91 model_repository_manager.cc:1352] successfully loaded 'ncf_food_model' version 1\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:44.338317 91 model_repository_manager.cc:1151] TriggerNextAction() 'ncf_food_model' version 1: 0\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:44.338336 91 model_repository_manager.cc:1165] no next action, trigger OnComplete()\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:44.338388 91 model_repository_manager.cc:728] VersionStates() 'ncf_food_model'\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:44.338422 91 model_repository_manager.cc:728] VersionStates() 'ncf_food_model'\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:44.338468 91 server.cc:559] \n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m +------------------+------+\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m | Repository Agent | Path |\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m +------------------+------+\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m +------------------+------+\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m \n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:44.338521 91 server.cc:586] \n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m +---------+---------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m | Backend | Path                                                    | Config                                                                                                                                                        |\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m +---------+---------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m | pytorch | /opt/tritonserver/backends/pytorch/libtriton_pytorch.so | {\"cmdline\":{\"auto-complete-config\":\"true\",\"min-compute-capability\":\"6.000000\",\"backend-directory\":\"/opt/tritonserver/backends\",\"default-max-batch-size\":\"4\"}} |\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m +---------+---------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m \n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:44.338545 91 model_repository_manager.cc:704] ModelStates()\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:44.340544 91 server.cc:629] \n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m +----------------+---------+--------+\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m | Model          | Version | Status |\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m +----------------+---------+--------+\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m | ncf_food_model | 1       | READY  |\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m +----------------+---------+--------+\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m \n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:44.340672 91 tritonserver.cc:2176] \n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m +----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m | Option                           | Value                                                                                                                                                                                        |\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m +----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m | server_id                        | triton                                                                                                                                                                                       |\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m | server_version                   | 2.24.0                                                                                                                                                                                       |\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m | server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics trace |\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m | model_repository_path[0]         | /opt/ml/model/                                                                                                                                                                               |\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m | model_control_mode               | MODE_EXPLICIT                                                                                                                                                                                |\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m | startup_models_0                 | ncf_food_model                                                                                                                                                                               |\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m | strict_model_config              | 0                                                                                                                                                                                            |\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m | rate_limit                       | OFF                                                                                                                                                                                          |\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m | pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                    |\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m | cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                                     |\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m | cuda_memory_pool_byte_size{1}    | 67108864                                                                                                                                                                                     |\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m | cuda_memory_pool_byte_size{2}    | 67108864                                                                                                                                                                                     |\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m | cuda_memory_pool_byte_size{3}    | 67108864                                                                                                                                                                                     |\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m | response_cache_byte_size         | 0                                                                                                                                                                                            |\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m | min_supported_compute_capability | 6.0                                                                                                                                                                                          |\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m | strict_readiness                 | 1                                                                                                                                                                                            |\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m | exit_timeout                     | 30                                                                                                                                                                                           |\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m +----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m \n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:44.344933 91 sagemaker_server.cc:280] Started Sagemaker HTTPService at 0.0.0.0:8080\n",
      "!\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:48.356822 91 sagemaker_server.cc:190] SageMaker request: 0 /ping\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:48.356962 91 model_repository_manager.cc:704] ModelStates()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "local_predictor = local_pytorch_model.deploy(\n",
    "                           instance_type=instance_type, \n",
    "                           initial_instance_count=1, \n",
    "                           endpoint_name=endpoint_name,\n",
    "                           wait=True,\n",
    "                           log = False,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 추론 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "payload:  {'inputs': [{'name': 'INPUT__0', 'shape': [1, 100], 'datatype': 'INT32', 'data': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}, {'name': 'INPUT__1', 'shape': [1, 100], 'datatype': 'INT32', 'data': [[997, 48, 165, 243, 762, 44, 590, 402, 982, 80, 731, 720, 205, 498, 257, 988, 344, 267, 761, 668, 452, 654, 142, 766, 1, 281, 232, 168, 602, 455, 431, 734, 261, 125, 476, 958, 312, 434, 742, 695, 861, 672, 630, 685, 107, 755, 162, 223, 299, 250, 585, 438, 184, 661, 594, 458, 378, 2, 345, 570, 913, 925, 456, 951, 374, 247, 470, 594, 187, 894, 658, 144, 453, 817, 277, 402, 386, 36, 204, 932, 740, 423, 193, 689, 834, 926, 843, 380, 971, 798, 550, 437, 520, 40, 501, 676, 304, 508, 243, 12]]}]}\n"
     ]
    }
   ],
   "source": [
    "def create_sample_payload():\n",
    "    # user\n",
    "    user_np = np.zeros((1,100)).astype(np.int32)\n",
    "    # item\n",
    "    item_np = np.random.randint(low=1, high=1000, size=(1,100)).astype(np.int32)\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": [\n",
    "            {\"name\": \"INPUT__0\", \"shape\": [1,100], \n",
    "             \"datatype\": \"INT32\", \"data\": user_np.tolist()},\n",
    "            {\"name\": \"INPUT__1\", \"shape\": [1,100], \n",
    "             \"datatype\": \"INT32\", \"data\": item_np.tolist()},\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return payload\n",
    "\n",
    "payload = create_sample_payload()\n",
    "print(\"payload: \", payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:53.768023 91 sagemaker_server.cc:190] SageMaker request: 2 /invocations\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:53.768123 91 model_repository_manager.cc:773] GetModel() 'ncf_food_model' version -1\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:53.768151 91 model_repository_manager.cc:773] GetModel() 'ncf_food_model' version -1\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:53.769230 91 infer_request.cc:713] [request id: <id_unknown>] prepared: [0x0x7f5bd4004880] request id: , model: ncf_food_model, requested version: -1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 1, priority: 0, timeout (us): 0\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m original inputs:\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m [0x0x7f5bd4015278] input: INPUT__1, type: INT32, original shape: [1,100], batch + shape: [1,100], shape: [100]\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m [0x0x7f5bd4014e58] input: INPUT__0, type: INT32, original shape: [1,100], batch + shape: [1,100], shape: [100]\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m override inputs:\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m inputs:\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m [0x0x7f5bd4014e58] input: INPUT__0, type: INT32, original shape: [1,100], batch + shape: [1,100], shape: [100]\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m [0x0x7f5bd4015278] input: INPUT__1, type: INT32, original shape: [1,100], batch + shape: [1,100], shape: [100]\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m original requested outputs:\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m requested outputs:\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m OUTPUT__0\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m \n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:53.769368 91 libtorch.cc:2076] model ncf_food_model, instance ncf_food_model, executing 1 requests\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:53.769404 91 libtorch.cc:947] TRITONBACKEND_ModelExecute: Running ncf_food_model with 1 requests\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:53.769462 91 pinned_memory_manager.cc:161] pinned memory allocation: size 400, addr 0x7f5de8000090\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:53.769582 91 pinned_memory_manager.cc:161] pinned memory allocation: size 400, addr 0x7f5de8000230\n",
      "result : \u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:56.267913 91 infer_response.cc:167] add response output: output: OUTPUT__0, type: FP32, shape: [1,100,1]\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:56.267962 91 http_server.cc:1088] HTTP: unable to provide 'OUTPUT__0' in GPU, will use CPU\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:56.267974 91 http_server.cc:1108] HTTP using buffer for: 'OUTPUT__0', size: 400, addr: 0x7f5bbaee52a0\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:56.268003 91 pinned_memory_manager.cc:161] pinned memory allocation: size 400, addr 0x7f5de80003d0\n",
      " {'model_name': 'ncf_food_model', 'model_version': '1', 'outputs': [{'name': 'OUTPUT__0', 'datatype': 'FP32', 'shape': [1, 100, 1], 'data': [-4.839515209197998, 1.4735954999923706, -2.369269847869873, 0.5929858088493347, -1.3574786186218262, 2.9009170532226562, -4.5265092849731445, -3.1178770065307617, -1.8846498727798462, -2.0428261756896973, -1.0859817266464233, -0.7866543531417847, -0.5726983547210693, -3.2671961784362793, -2.586115837097168, -1.6596626043319702, -3.8252816200256348, -2.306828022003174, -3.1906089782714844, -3.2827811241149902, -0.7897399663925171, -2.908857822418213, -1.0965994596481323, 0.8126152753829956, 2.704719066619873, -1.0788018703460693, -3.399214267730713, 1.44561767578125, -1.3832101821899414, -1.9774476289749146, -1.0886330604553223, -2.0051159858703613, -3.70497989654541, -2.6708216667175293, -4.687002182006836, -2.047858715057373, -0.6502715349197388, 0.40011847019195557, -3.213282585144043, -0.9822622537612915, -2.574035167694092, -1.8052793741226196, -1.3300753831863403, -1.7501051425933838, -3.6292529106140137, 0.5414324998855591, -0.9934098720550537, -3.2402572631835938, -4.00281286239624, -4.75978422164917, 0.9719388484954834, 0.4256531596183777, 1.820107102394104, 1.0105868577957153, -2.031050682067871, -4.281177997589111, -3.8056793212890625, 2.2152700424194336, -0.6016783118247986, -1.9914065599441528, -4.476179122924805, -2.500044822692871, -4.376490592956543, -3.9181461334228516, -1.4478126764297485, -1.9369317293167114, -0.36356621980667114, -2.031050682067871, -1.8983184099197388, -3.4653892517089844, -4.177432060241699, -3.3410496711730957, -0.7753016352653503, -5.31236457824707, 0.6453456878662109, -3.1178770065307617, -0.6773332953453064, -3.1025280952453613, -1.0724939107894897, -0.9892745018005371, -1.2057181596755981, -1.738818645477295, -1.712149739265442, -2.9242286682128906, -1.5009523630142212, -2.823000431060791, -1.638752818107605, -3.0297131538391113, -3.483567714691162, 1.4750019311904907, -4.110897541046143, 1.7976950407028198, -0.9720678329467773, 3.8185200691223145, -1.8937138319015503, -0.06632408499717712, -3.1129817962646484, -2.169764518737793, 0.5929858088493347, -0.6745018362998962]}]}\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:56.268061 91 pinned_memory_manager.cc:190] pinned memory deallocation: addr 0x7f5de80003d0\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:56.268168 91 http_server.cc:1182] HTTP release: size 400, addr 0x7f5bbaee52a0\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:56.268216 91 pinned_memory_manager.cc:190] pinned memory deallocation: addr 0x7f5de8000090\n",
      "\u001b[36mholdgwluib-algo-1-5toty |\u001b[0m I1211 07:57:56.268243 91 pinned_memory_manager.cc:190] pinned memory deallocation: addr 0x7f5de8000230\n"
     ]
    }
   ],
   "source": [
    "def single_model_invoke_endpoint(client,endpoint_name, payload): \n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/octet-stream\", \n",
    "        Body=json.dumps(payload),\n",
    "    )\n",
    "\n",
    "    result = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "    \n",
    "    return result\n",
    "\n",
    "runtime_client = sagemaker.local.LocalSagemakerRuntimeClient()    \n",
    "result = single_model_invoke_endpoint(runtime_client,endpoint_name, payload)\n",
    "print(\"result : \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5. 로컬 앤드포인트 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_utils import delete_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gracefully stopping... (press Ctrl+C again to force)\n",
      "--- Deleted model: sagemaker-tritonserver-2022-12-11-07-57-32-571\n",
      "--- Deleted endpoint_config: triton-ncf-ep-2022-12-11-07-57-19\n",
      "--- Deleted endpoint: triton-ncf-ep-2022-12-11-07-57-19\n"
     ]
    }
   ],
   "source": [
    "client = sagemaker.local.LocalSagemakerClient()\n",
    "delete_endpoint(client, endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 클라우드 배포"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. 변수 및 컨테이너 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model_name = f\"{prefix}-mdl-{ts}\"\n",
    "real_endpoint_config_name = f\"{prefix}-epc-{ts}\"\n",
    "real_endpoint_name = f\"{prefix}-ep-{ts}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = {\"Image\": mme_triton_image_uri,\n",
    "             \"ModelDataUrl\": model_uri_pt}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "container:  {'Image': '785573368785.dkr.ecr.us-east-1.amazonaws.com/sagemaker-tritonserver:22.07-py3', 'ModelDataUrl': 's3://sagemaker-us-east-1-057716757052/triton-ncf/ncf_food_model.model.tar.gz'}\n",
      "sm_model_name:  triton-ncf-mdl-2022-12-11-07-57-19\n"
     ]
    }
   ],
   "source": [
    "print(\"container: \", container)\n",
    "print(\"sm_model_name: \", sm_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. 세이지 메이커 모델, 앤드포인트 컨피그, 앤드포인트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Arn: arn:aws:sagemaker:us-east-1:057716757052:model/triton-ncf-mdl-2022-12-11-07-57-19\n"
     ]
    }
   ],
   "source": [
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-1:057716757052:endpoint-config/triton-ncf-epc-2022-12-11-07-57-19\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=real_endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g4dn.4xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Arn: arn:aws:sagemaker:us-east-1:057716757052:endpoint/triton-ncf-ep-2022-12-11-07-57-19\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=real_endpoint_name, EndpointConfigName= real_endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-east-1:057716757052:endpoint/triton-ncf-ep-2022-12-11-07-57-19\n",
      "Status: InService\n",
      "CPU times: user 76 ms, sys: 9.44 ms, total: 85.4 ms\n",
      "Wall time: 5min\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName= real_endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=real_endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'ncf_food_model',\n",
       " 'model_version': '1',\n",
       " 'outputs': [{'name': 'OUTPUT__0',\n",
       "   'datatype': 'FP32',\n",
       "   'shape': [1, 100, 1],\n",
       "   'data': [-4.839515209197998,\n",
       "    1.4735954999923706,\n",
       "    -2.369269847869873,\n",
       "    0.5929858088493347,\n",
       "    -1.3574788570404053,\n",
       "    2.9009170532226562,\n",
       "    -4.5265092849731445,\n",
       "    -3.1178770065307617,\n",
       "    -1.8846498727798462,\n",
       "    -2.0428261756896973,\n",
       "    -1.0859817266464233,\n",
       "    -0.7866544723510742,\n",
       "    -0.5726983547210693,\n",
       "    -3.2671961784362793,\n",
       "    -2.586115837097168,\n",
       "    -1.6596626043319702,\n",
       "    -3.8252816200256348,\n",
       "    -2.306828022003174,\n",
       "    -3.1906089782714844,\n",
       "    -3.2827816009521484,\n",
       "    -0.7897400856018066,\n",
       "    -2.908857822418213,\n",
       "    -1.0965995788574219,\n",
       "    0.8126152753829956,\n",
       "    2.704719066619873,\n",
       "    -1.0788019895553589,\n",
       "    -3.399214267730713,\n",
       "    1.44561767578125,\n",
       "    -1.383210301399231,\n",
       "    -1.9774476289749146,\n",
       "    -1.0886330604553223,\n",
       "    -2.0051159858703613,\n",
       "    -3.70497989654541,\n",
       "    -2.6708216667175293,\n",
       "    -4.687002182006836,\n",
       "    -2.047858953475952,\n",
       "    -0.6502715349197388,\n",
       "    0.40011847019195557,\n",
       "    -3.213282585144043,\n",
       "    -0.982262134552002,\n",
       "    -2.574035167694092,\n",
       "    -1.8052793741226196,\n",
       "    -1.3300753831863403,\n",
       "    -1.7501051425933838,\n",
       "    -3.6292529106140137,\n",
       "    0.5414324998855591,\n",
       "    -0.9934098720550537,\n",
       "    -3.2402572631835938,\n",
       "    -4.002812385559082,\n",
       "    -4.75978422164917,\n",
       "    0.9719388484954834,\n",
       "    0.4256531596183777,\n",
       "    1.820107102394104,\n",
       "    1.0105867385864258,\n",
       "    -2.031050682067871,\n",
       "    -4.281177997589111,\n",
       "    -3.8056793212890625,\n",
       "    2.2152700424194336,\n",
       "    -0.6016783714294434,\n",
       "    -1.9914066791534424,\n",
       "    -4.476179122924805,\n",
       "    -2.500044822692871,\n",
       "    -4.376491069793701,\n",
       "    -3.9181461334228516,\n",
       "    -1.4478129148483276,\n",
       "    -1.9369319677352905,\n",
       "    -0.36356624960899353,\n",
       "    -2.031050682067871,\n",
       "    -1.8983184099197388,\n",
       "    -3.4653892517089844,\n",
       "    -4.177432060241699,\n",
       "    -3.3410496711730957,\n",
       "    -0.775301456451416,\n",
       "    -5.31236457824707,\n",
       "    0.6453458070755005,\n",
       "    -3.1178770065307617,\n",
       "    -0.6773331761360168,\n",
       "    -3.1025280952453613,\n",
       "    -1.0724937915802002,\n",
       "    -0.9892745018005371,\n",
       "    -1.2057181596755981,\n",
       "    -1.7388187646865845,\n",
       "    -1.712149977684021,\n",
       "    -2.924229145050049,\n",
       "    -1.5009523630142212,\n",
       "    -2.823000431060791,\n",
       "    -1.638752818107605,\n",
       "    -3.0297131538391113,\n",
       "    -3.483567714691162,\n",
       "    1.4750016927719116,\n",
       "    -4.110897541046143,\n",
       "    1.7976950407028198,\n",
       "    -0.9720678329467773,\n",
       "    3.8185200691223145,\n",
       "    -1.8937135934829712,\n",
       "    -0.06632408499717712,\n",
       "    -3.1129817962646484,\n",
       "    -2.169764518737793,\n",
       "    0.5929858088493347,\n",
       "    -0.6745019555091858]}]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runtime_client = boto3.Session().client('sagemaker-runtime')\n",
    "single_model_invoke_endpoint(runtime_client,real_endpoint_name, payload)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 앤드포인트 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Deleted model: triton-ncf-mdl-2022-12-11-07-57-19\n",
      "--- Deleted endpoint_config: triton-ncf-epc-2022-12-11-07-57-19\n",
      "--- Deleted endpoint: triton-ncf-ep-2022-12-11-07-57-19\n"
     ]
    }
   ],
   "source": [
    "client = boto3.Session().client('sagemaker')\n",
    "delete_endpoint(client, real_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
