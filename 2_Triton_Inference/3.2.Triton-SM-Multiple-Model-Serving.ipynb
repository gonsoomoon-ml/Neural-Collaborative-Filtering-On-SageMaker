{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [모듈 3.2] 2개의 NCF 모델을 SageMaker Endpoint Triton 서빙"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 환경 셋업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1. 기본 세팅\n",
    "사용하는 패키지는 import 시점에 다시 재로딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('./src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전 노트북에서 훈련 후의 아티펙트를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r model_serving_folder\n",
    "%store -r food_model_name\n",
    "%store -r fashion_model_name\n",
    "%store -r bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "import boto3, json, sagemaker, time\n",
    "import numpy as np\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"triton-ncf\"\n",
    "\n",
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "# endpoint variables\n",
    "sm_model_name = f\"{prefix}-mdl-{ts}\"\n",
    "endpoint_config_name = f\"{prefix}-epc-{ts}\"\n",
    "endpoint_name = f\"{prefix}-ep-{ts}\"\n",
    "model_data_url = f\"s3://{bucket}/{prefix}/\"\n",
    "instance_type = \"local_gpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sm_model_name: \n",
      " triton-ncf-mdl-2022-12-11-08-36-49\n",
      "endpoint_config_name: \n",
      " triton-ncf-epc-2022-12-11-08-36-49\n",
      "endpoint_name: \n",
      " triton-ncf-ep-2022-12-11-08-36-49\n"
     ]
    }
   ],
   "source": [
    "print(\"sm_model_name: \\n\", sm_model_name)\n",
    "print(\"endpoint_config_name: \\n\", endpoint_config_name)\n",
    "print(\"endpoint_name: \\n\", endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triton Docker Image 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mme_triton_image_uri: \n",
      " 785573368785.dkr.ecr.us-east-1.amazonaws.com/sagemaker-tritonserver:22.07-py3\n"
     ]
    }
   ],
   "source": [
    "from triton_util import account_id_map\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "mme_triton_image_uri = (\n",
    "    \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.07-py3\".format(\n",
    "        account_id=account_id_map[region], region=region, base=base\n",
    "    )\n",
    ")\n",
    "print(\"mme_triton_image_uri: \\n\", mme_triton_image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 모델 패키징 (model.tar.gz) 및 S3 업로딩\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from triton_util import tar_artifact, upload_tar_s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Food ncf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxrwxr-x ec2-user/ec2-user 0 2022-12-11 08:15 ncf_food_model/\n",
      "-rw-rw-r-- ec2-user/ec2-user 306 2022-12-11 08:20 ncf_food_model/config.pbtxt\n",
      "drwxrwxr-x ec2-user/ec2-user   0 2022-12-11 08:15 ncf_food_model/1/\n",
      "-rw-rw-r-- ec2-user/ec2-user 6440569 2022-12-11 08:20 ncf_food_model/1/model.pt\n",
      "food_model_tar_file:  ncf_food_model.model.tar.gz\n",
      "food_model_uri_pt:  s3://sagemaker-us-east-1-057716757052/triton-ncf/ncf_food_model.model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "food_model_tar_file = tar_artifact(model_serving_folder, food_model_name)    \n",
    "print(\"food_model_tar_file: \", food_model_tar_file)\n",
    "food_model_uri_pt = upload_tar_s3(sagemaker_session, food_model_tar_file, prefix)\n",
    "print(\"food_model_uri_pt: \", food_model_uri_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Fashion ncf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxrwxr-x ec2-user/ec2-user 0 2022-12-11 08:15 ncf_fashion_model/\n",
      "-rw-rw-r-- ec2-user/ec2-user 309 2022-12-11 08:20 ncf_fashion_model/config.pbtxt\n",
      "drwxrwxr-x ec2-user/ec2-user   0 2022-12-11 08:15 ncf_fashion_model/1/\n",
      "-rw-rw-r-- ec2-user/ec2-user 6441610 2022-12-11 08:20 ncf_fashion_model/1/model.pt\n",
      "fashion_model_tar_file:  ncf_fashion_model.model.tar.gz\n",
      "fashion_model_uri_pt:  s3://sagemaker-us-east-1-057716757052/triton-ncf/ncf_fashion_model.model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "fashion_model_tar_file = tar_artifact(model_serving_folder, fashion_model_name)    \n",
    "print(\"fashion_model_tar_file: \", fashion_model_tar_file)\n",
    "fashion_model_uri_pt = upload_tar_s3(sagemaker_session, fashion_model_tar_file, prefix)\n",
    "print(\"fashion_model_uri_pt: \", fashion_model_uri_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 클라우드 배포"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. 변수 및 컨테이너 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model_name = f\"{prefix}-mdl-{ts}\"\n",
    "real_endpoint_config_name = f\"{prefix}-epc-{ts}\"\n",
    "real_endpoint_name = f\"{prefix}-ep-{ts}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = {\"Image\": mme_triton_image_uri, \n",
    "             \"ModelDataUrl\": model_data_url, \n",
    "             \"Mode\": \"MultiModel\",\n",
    "             'Environment' : {\n",
    "                                \"SAGEMAKER_TRITON_LOG_VERBOSE\": \"3\",\n",
    "                                \"SAGEMAKER_TRITON_LOG_INFO\": \"1\",\n",
    "                                \"SAGEMAKER_TRITON_LOG_WARNING\" : \"1\",\n",
    "                                \"SAGEMAKER_TRITON_LOG_ERROR\" : \"1\"\n",
    "                             }             \n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "container:  {'Image': '785573368785.dkr.ecr.us-east-1.amazonaws.com/sagemaker-tritonserver:22.07-py3', 'ModelDataUrl': 's3://sagemaker-us-east-1-057716757052/triton-ncf/', 'Mode': 'MultiModel', 'Environment': {'SAGEMAKER_TRITON_LOG_VERBOSE': '3', 'SAGEMAKER_TRITON_LOG_INFO': '1', 'SAGEMAKER_TRITON_LOG_WARNING': '1', 'SAGEMAKER_TRITON_LOG_ERROR': '1'}}\n",
      "sm_model_name:  triton-ncf-mdl-2022-12-11-08-36-49\n"
     ]
    }
   ],
   "source": [
    "print(\"container: \", container)\n",
    "print(\"sm_model_name: \", sm_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. 세이지 메이커 모델, 앤드포인트 컨피그, 앤드포인트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Arn: arn:aws:sagemaker:us-east-1:057716757052:model/triton-ncf-mdl-2022-12-11-08-36-49\n"
     ]
    }
   ],
   "source": [
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-1:057716757052:endpoint-config/triton-ncf-epc-2022-12-11-08-36-49\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=real_endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g4dn.4xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Arn: arn:aws:sagemaker:us-east-1:057716757052:endpoint/triton-ncf-ep-2022-12-11-08-36-49\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=real_endpoint_name, EndpointConfigName= real_endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-east-1:057716757052:endpoint/triton-ncf-ep-2022-12-11-08-36-49\n",
      "Status: InService\n",
      "CPU times: user 73.2 ms, sys: 8.79 ms, total: 82 ms\n",
      "Wall time: 5min\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName= real_endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=real_endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.3. GPU 다중 모델 엔드포인트에 대한 자동 확장 정책 설정\n",
    "\n",
    "Amazon SageMaker 다중 모델 엔드포인트는 호스팅된 모델에 대해 자동 조정(Auto Scaling)을 지원합니다. Auto Scaling은 워크로드의 변화에 ​​따라 모델에 대해 프로비저닝된 인스턴스 수를 동적으로 조정합니다. 워크로드가 증가하면 Auto Scaling이 더 많은 인스턴스를 온라인 상태로 만듭니다. 워크로드가 감소하면 Auto Scaling이 불필요한 인스턴스를 제거하므로 사용하지 않는 프로비저닝된 인스턴스에 대해 비용을 지불하지 않아도 됩니다.\n",
    "\n",
    "아래 조정 정책에서 TargetTrackingScalingPolicyConfiguration 구성의 사용자 지정 지표 GPUUtilization을 사용하고 해당 지표의 대상 값에 대해 TargetValue를 60.0으로 설정합니다. 이 자동 확장 정책은 GPU 사용률이 60% 이상일 때 MaxCapacity까지 추가 인스턴스를 프로비저닝합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform auto-scaling of the endpoint based on GPU memory utilization\n",
    "# This is the format in which application autoscaling references the endpoint\n",
    "auto_scaling_client = boto3.client(\"application-autoscaling\")\n",
    "\n",
    "resource_id = \"endpoint/\" + endpoint_name + \"/variant/\" + \"AllTraffic\"\n",
    "response = auto_scaling_client.register_scalable_target(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=5,\n",
    ")\n",
    "\n",
    "\n",
    "# GPUMemoryUtilization metric\n",
    "response = auto_scaling_client.put_scaling_policy(\n",
    "    PolicyName=\"GPUUtil-ScalingPolicy\",\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",  # SageMaker supports only Instance Count\n",
    "    PolicyType=\"TargetTrackingScaling\",  # 'StepScaling'|'TargetTrackingScaling'\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        # Scale out when GPU utilization hits GPUUtilization target value.\n",
    "        \"TargetValue\": 60.0,\n",
    "        \"CustomizedMetricSpecification\": {\n",
    "            \"MetricName\": \"GPUUtilization\",\n",
    "            \"Namespace\": \"/aws/sagemaker/Endpoints\",\n",
    "            \"Dimensions\": [\n",
    "                {\"Name\": \"EndpointName\", \"Value\": endpoint_name},\n",
    "                {\"Name\": \"VariantName\", \"Value\": \"AllTraffic\"},\n",
    "            ],\n",
    "            \"Statistic\": \"Average\",  # Possible - 'Statistic': 'Average'|'Minimum'|'Maximum'|'SampleCount'|'Sum'\n",
    "            \"Unit\": \"Percent\",\n",
    "        },\n",
    "        \"ScaleInCooldown\": 600,\n",
    "        \"ScaleOutCooldown\": 200,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. SageMaker Endpoint 에 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. 샘플 입력 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "payload:  {'inputs': [{'name': 'INPUT__0', 'shape': [1, 100], 'datatype': 'INT32', 'data': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}, {'name': 'INPUT__1', 'shape': [1, 100], 'datatype': 'INT32', 'data': [[121, 418, 61, 711, 410, 234, 718, 726, 53, 486, 596, 953, 800, 148, 628, 838, 90, 333, 588, 554, 380, 24, 515, 990, 222, 172, 845, 866, 123, 923, 580, 339, 895, 865, 794, 407, 461, 34, 431, 303, 635, 877, 346, 304, 58, 740, 138, 631, 572, 100, 995, 388, 29, 826, 288, 284, 274, 807, 400, 716, 426, 444, 866, 409, 548, 90, 434, 434, 221, 297, 153, 936, 964, 124, 918, 256, 399, 400, 526, 909, 182, 424, 197, 593, 105, 678, 661, 242, 44, 703, 671, 747, 774, 269, 433, 859, 630, 222, 776, 645]]}]}\n"
     ]
    }
   ],
   "source": [
    "def create_sample_payload():\n",
    "    # user\n",
    "    user_np = np.zeros((1,100)).astype(np.int32)\n",
    "    # item\n",
    "    item_np = np.random.randint(low=1, high=1000, size=(1,100)).astype(np.int32)\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": [\n",
    "            {\"name\": \"INPUT__0\", \"shape\": [1,100], \n",
    "             \"datatype\": \"INT32\", \"data\": user_np.tolist()},\n",
    "            {\"name\": \"INPUT__1\", \"shape\": [1,100], \n",
    "             \"datatype\": \"INT32\", \"data\": item_np.tolist()},\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return payload\n",
    "\n",
    "payload = create_sample_payload()\n",
    "print(\"payload: \", payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. NCF Food 모델에 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_model_invoke_endpoint(client,endpoint_name, payload, TargetModel): \n",
    "    print(\"Model: \", TargetModel)\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/octet-stream\", \n",
    "        Body=json.dumps(payload),\n",
    "        TargetModel= TargetModel,        \n",
    "    )\n",
    "\n",
    "    result = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncf_food_model.model.tar.gz\n",
      "ncf_fashion_model.model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(food_model_tar_file)\n",
    "print(fashion_model_tar_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  ncf_food_model.model.tar.gz\n",
      "--- 2.7085461616516113 seconds ---\n",
      "result:  {'model_name': '902abbd0beb47e68e0c5bb507b705b2f', 'model_version': '1', 'outputs': [{'name': 'OUTPUT__0', 'datatype': 'FP32', 'shape': [1, 100, 1], 'data': [-2.6314568519592285, -1.2007676362991333, -1.626147985458374, -0.978064775466919, -4.502701759338379, -5.763144493103027, 0.0025492608547210693, 2.2988052368164062, -0.36251702904701233, -1.389423131942749, 2.1538500785827637, -4.709161281585693, -1.9165270328521729, -0.42308861017227173, -1.6481953859329224, -4.8860673904418945, -2.790921211242676, -3.4214940071105957, 1.8567599058151245, -6.499532222747803, -3.0297131538391113, -0.8539845943450928, -0.3104788064956665, -3.6795530319213867, -3.3094944953918457, -4.160562515258789, -3.5592541694641113, -1.2561880350112915, 1.2782834768295288, 1.8965405225753784, 1.4852981567382812, -2.4008069038391113, -1.3010913133621216, -0.872534453868866, -1.1580091714859009, -3.3750433921813965, -2.2550907135009766, 2.12669038772583, -1.0886330604553223, -1.9066804647445679, 0.3665884733200073, -2.759995937347412, -1.8128150701522827, -3.1129817962646484, -0.20609892904758453, -1.2057181596755981, -1.3146790266036987, -1.2153445482254028, -2.4049553871154785, -3.3287649154663086, -4.128378868103027, -0.3377791941165924, 1.7573786973953247, -3.242410182952881, -1.9733240604400635, -2.231464385986328, -1.82097327709198, -2.1065564155578613, 0.7042698860168457, -0.4936055541038513, 0.20366978645324707, 0.4498709440231323, -1.2561880350112915, -3.322178363800049, -1.628231406211853, -2.790921211242676, 0.40011847019195557, 0.40011847019195557, 1.539404034614563, -4.9757914543151855, 1.122054934501648, -3.4640512466430664, -2.7703022956848145, 2.524380683898926, -2.374065399169922, -0.2614613175392151, -1.8214789628982544, 0.7042698860168457, -2.9881014823913574, -2.9292893409729004, -0.9610579013824463, 1.2642154693603516, -4.105822563171387, -0.4247943162918091, 2.0341691970825195, 0.3428676128387451, 1.0105867385864258, -0.545168399810791, 2.9009170532226562, -1.730054259300232, 0.41685885190963745, -1.9501492977142334, -2.175678253173828, -2.427724838256836, -1.9859875440597534, -5.05520486831665, -1.3300753831863403, -3.3094944953918457, -2.6792922019958496, -0.632525622844696]}]}\n"
     ]
    }
   ],
   "source": [
    "runtime_client = boto3.Session().client('sagemaker-runtime')\n",
    "start_time = time.time()\n",
    "result = multiple_model_invoke_endpoint(runtime_client,endpoint_name, payload, food_model_tar_file)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('result: ', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker Endpoint 의 로그를 Cloud Watch 를 통해서 화인 함.\n",
    "Input, Output 의 입력 구조 및 어떠한 \"모델\"이 사용이 되었는지, 그리고 Memory 할당, 해제를 화인 할 수 있습니다.\n",
    "- 아래 메세지 \"http_server.cc:1088] HTTP: unable to provide 'OUTPUT__0' in GPU, will use CPU\" 는 에러가 아닙니다. 아래 내용 참조 하세요.\n",
    "    - this log comes from the HTTP / gRPC server. It's not an error. Since the output is returned via HTTP/GRPC the buffer resides on CPU not GPU even though your model outputs may have been on GPU.\n",
    "        - https://github.com/triton-inference-server/server/issues/2090\n",
    "\n",
    "![cloud_watch_food_log.png](img/cloud_watch_food_log.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.NCF Fashion 모델에 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  ncf_fashion_model.model.tar.gz\n",
      "--- 0.4097862243652344 seconds ---\n",
      "result:  {'model_name': 'af6f3fa40112f0758f09d331379c0b98', 'model_version': '1', 'outputs': [{'name': 'OUTPUT__0', 'datatype': 'FP32', 'shape': [1, 100, 1], 'data': [-2.6314568519592285, -1.2007676362991333, -1.626147985458374, -0.978064775466919, -4.502701759338379, -5.763144493103027, 0.0025492608547210693, 2.2988052368164062, -0.36251702904701233, -1.389423131942749, 2.1538500785827637, -4.709161281585693, -1.9165270328521729, -0.42308861017227173, -1.6481953859329224, -4.8860673904418945, -2.790921211242676, -3.4214940071105957, 1.8567599058151245, -6.499532222747803, -3.0297131538391113, -0.8539845943450928, -0.3104788064956665, -3.6795530319213867, -3.3094944953918457, -4.160562515258789, -3.5592541694641113, -1.2561880350112915, 1.2782834768295288, 1.8965405225753784, 1.4852981567382812, -2.4008069038391113, -1.3010913133621216, -0.872534453868866, -1.1580091714859009, -3.3750433921813965, -2.2550907135009766, 2.12669038772583, -1.0886330604553223, -1.9066804647445679, 0.3665884733200073, -2.759995937347412, -1.8128150701522827, -3.1129817962646484, -0.20609892904758453, -1.2057181596755981, -1.3146790266036987, -1.2153445482254028, -2.4049553871154785, -3.3287649154663086, -4.128378868103027, -0.3377791941165924, 1.7573786973953247, -3.242410182952881, -1.9733240604400635, -2.231464385986328, -1.82097327709198, -2.1065564155578613, 0.7042698860168457, -0.4936055541038513, 0.20366978645324707, 0.4498709440231323, -1.2561880350112915, -3.322178363800049, -1.628231406211853, -2.790921211242676, 0.40011847019195557, 0.40011847019195557, 1.539404034614563, -4.9757914543151855, 1.122054934501648, -3.4640512466430664, -2.7703022956848145, 2.524380683898926, -2.374065399169922, -0.2614613175392151, -1.8214789628982544, 0.7042698860168457, -2.9881014823913574, -2.9292893409729004, -0.9610579013824463, 1.2642154693603516, -4.105822563171387, -0.4247943162918091, 2.0341691970825195, 0.3428676128387451, 1.0105867385864258, -0.545168399810791, 2.9009170532226562, -1.730054259300232, 0.41685885190963745, -1.9501492977142334, -2.175678253173828, -2.427724838256836, -1.9859875440597534, -5.05520486831665, -1.3300753831863403, -3.3094944953918457, -2.6792922019958496, -0.632525622844696]}]}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "runtime_client = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "start_time = time.time()\n",
    "result = multiple_model_invoke_endpoint(runtime_client,endpoint_name, payload, fashion_model_tar_file)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('result: ', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 앤드포인트 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Deleted model: triton-ncf-mdl-2022-12-11-08-36-49\n",
      "--- Deleted endpoint_config: triton-ncf-epc-2022-12-11-08-36-49\n",
      "--- Deleted endpoint: triton-ncf-ep-2022-12-11-08-36-49\n"
     ]
    }
   ],
   "source": [
    "from inference_utils import delete_endpoint\n",
    "\n",
    "client = boto3.Session().client('sagemaker')\n",
    "delete_endpoint(client, real_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/triton-inference-server/server/issues/2090\n",
    "\n",
    "this log comes from the HTTP / gRPC server. It's not an error. Since the output is returned via HTTP/GRPC the buffer resides on CPU not GPU even though your model outputs may have been on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
