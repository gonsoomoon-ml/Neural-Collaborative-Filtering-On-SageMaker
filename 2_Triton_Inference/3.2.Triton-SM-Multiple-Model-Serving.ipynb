{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [모듈 3.2] 2개의 NCF 모델을 SageMaker Endpoint Triton 서빙"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 환경 셋업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1. 기본 세팅\n",
    "사용하는 패키지는 import 시점에 다시 재로딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('./src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전 노트북에서 훈련 후의 아티펙트를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r model_serving_folder\n",
    "%store -r food_model_name\n",
    "%store -r fashion_model_name\n",
    "%store -r bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "import boto3, json, sagemaker, time\n",
    "import numpy as np\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"triton-ncf\"\n",
    "\n",
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "# endpoint variables\n",
    "sm_model_name = f\"{prefix}-mdl-{ts}\"\n",
    "endpoint_config_name = f\"{prefix}-epc-{ts}\"\n",
    "endpoint_name = f\"{prefix}-ep-{ts}\"\n",
    "model_data_url = f\"s3://{bucket}/{prefix}/\"\n",
    "instance_type = \"local_gpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sm_model_name: \\n\", sm_model_name)\n",
    "print(\"endpoint_config_name: \\n\", endpoint_config_name)\n",
    "print(\"endpoint_name: \\n\", endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triton Docker Image 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from triton_util import account_id_map\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "mme_triton_image_uri = (\n",
    "    \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.07-py3\".format(\n",
    "        account_id=account_id_map[region], region=region, base=base\n",
    "    )\n",
    ")\n",
    "print(\"mme_triton_image_uri: \\n\", mme_triton_image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 모델 패키징 (model.tar.gz) 및 S3 업로딩\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from triton_util import tar_artifact, upload_tar_s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Food ncf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_model_tar_file = tar_artifact(model_serving_folder, food_model_name)    \n",
    "print(\"food_model_tar_file: \", food_model_tar_file)\n",
    "food_model_uri_pt = upload_tar_s3(sagemaker_session, food_model_tar_file, prefix)\n",
    "print(\"food_model_uri_pt: \", food_model_uri_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Fashion ncf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_model_tar_file = tar_artifact(model_serving_folder, fashion_model_name)    \n",
    "print(\"fashion_model_tar_file: \", fashion_model_tar_file)\n",
    "fashion_model_uri_pt = upload_tar_s3(sagemaker_session, fashion_model_tar_file, prefix)\n",
    "print(\"fashion_model_uri_pt: \", fashion_model_uri_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 클라우드 배포"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. 변수 및 컨테이너 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model_name = f\"{prefix}-mdl-{ts}\"\n",
    "real_endpoint_config_name = f\"{prefix}-epc-{ts}\"\n",
    "real_endpoint_name = f\"{prefix}-ep-{ts}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = {\"Image\": mme_triton_image_uri, \n",
    "             \"ModelDataUrl\": model_data_url, \n",
    "             \"Mode\": \"MultiModel\",\n",
    "             'Environment' : {\n",
    "                                \"SAGEMAKER_TRITON_LOG_VERBOSE\": \"3\",\n",
    "                                \"SAGEMAKER_TRITON_LOG_INFO\": \"1\",\n",
    "                                \"SAGEMAKER_TRITON_LOG_WARNING\" : \"1\",\n",
    "                                \"SAGEMAKER_TRITON_LOG_ERROR\" : \"1\"\n",
    "                             }             \n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"container: \", container)\n",
    "print(\"sm_model_name: \", sm_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. 세이지 메이커 모델, 앤드포인트 컨피그, 앤드포인트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=real_endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g4dn.4xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=real_endpoint_name, EndpointConfigName= real_endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName= real_endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=real_endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.3. GPU 다중 모델 엔드포인트에 대한 자동 확장 정책 설정\n",
    "\n",
    "Amazon SageMaker 다중 모델 엔드포인트는 호스팅된 모델에 대해 자동 조정(Auto Scaling)을 지원합니다. Auto Scaling은 워크로드의 변화에 ​​따라 모델에 대해 프로비저닝된 인스턴스 수를 동적으로 조정합니다. 워크로드가 증가하면 Auto Scaling이 더 많은 인스턴스를 온라인 상태로 만듭니다. 워크로드가 감소하면 Auto Scaling이 불필요한 인스턴스를 제거하므로 사용하지 않는 프로비저닝된 인스턴스에 대해 비용을 지불하지 않아도 됩니다.\n",
    "\n",
    "아래 조정 정책에서 TargetTrackingScalingPolicyConfiguration 구성의 사용자 지정 지표 GPUUtilization을 사용하고 해당 지표의 대상 값에 대해 TargetValue를 60.0으로 설정합니다. 이 자동 확장 정책은 GPU 사용률이 60% 이상일 때 MaxCapacity까지 추가 인스턴스를 프로비저닝합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform auto-scaling of the endpoint based on GPU memory utilization\n",
    "# This is the format in which application autoscaling references the endpoint\n",
    "auto_scaling_client = boto3.client(\"application-autoscaling\")\n",
    "\n",
    "resource_id = \"endpoint/\" + endpoint_name + \"/variant/\" + \"AllTraffic\"\n",
    "response = auto_scaling_client.register_scalable_target(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=5,\n",
    ")\n",
    "\n",
    "\n",
    "# GPUMemoryUtilization metric\n",
    "response = auto_scaling_client.put_scaling_policy(\n",
    "    PolicyName=\"GPUUtil-ScalingPolicy\",\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",  # SageMaker supports only Instance Count\n",
    "    PolicyType=\"TargetTrackingScaling\",  # 'StepScaling'|'TargetTrackingScaling'\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        # Scale out when GPU utilization hits GPUUtilization target value.\n",
    "        \"TargetValue\": 60.0,\n",
    "        \"CustomizedMetricSpecification\": {\n",
    "            \"MetricName\": \"GPUUtilization\",\n",
    "            \"Namespace\": \"/aws/sagemaker/Endpoints\",\n",
    "            \"Dimensions\": [\n",
    "                {\"Name\": \"EndpointName\", \"Value\": endpoint_name},\n",
    "                {\"Name\": \"VariantName\", \"Value\": \"AllTraffic\"},\n",
    "            ],\n",
    "            \"Statistic\": \"Average\",  # Possible - 'Statistic': 'Average'|'Minimum'|'Maximum'|'SampleCount'|'Sum'\n",
    "            \"Unit\": \"Percent\",\n",
    "        },\n",
    "        \"ScaleInCooldown\": 600,\n",
    "        \"ScaleOutCooldown\": 200,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. SageMaker Endpoint 에 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. 샘플 입력 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_payload():\n",
    "    # user\n",
    "    user_np = np.zeros((1,100)).astype(np.int32)\n",
    "    # item\n",
    "    item_np = np.random.randint(low=1, high=1000, size=(1,100)).astype(np.int32)\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": [\n",
    "            {\"name\": \"INPUT__0\", \"shape\": [1,100], \n",
    "             \"datatype\": \"INT32\", \"data\": user_np.tolist()},\n",
    "            {\"name\": \"INPUT__1\", \"shape\": [1,100], \n",
    "             \"datatype\": \"INT32\", \"data\": item_np.tolist()},\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return payload\n",
    "\n",
    "payload = create_sample_payload()\n",
    "print(\"payload: \", payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. NCF Food 모델에 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_model_invoke_endpoint(client,endpoint_name, payload, TargetModel): \n",
    "    print(\"Model: \", TargetModel)\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/octet-stream\", \n",
    "        Body=json.dumps(payload),\n",
    "        TargetModel= TargetModel,        \n",
    "    )\n",
    "\n",
    "    result = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(food_model_tar_file)\n",
    "print(fashion_model_tar_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runtime_client = boto3.Session().client('sagemaker-runtime')\n",
    "start_time = time.time()\n",
    "result = multiple_model_invoke_endpoint(runtime_client,endpoint_name, payload, food_model_tar_file)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('result: ', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker Endpoint 의 로그를 Cloud Watch 를 통해서 화인 함.\n",
    "Input, Output 의 입력 구조 및 어떠한 \"모델\"이 사용이 되었는지, 그리고 Memory 할당, 해제를 화인 할 수 있습니다.\n",
    "- 아래 메세지 \"http_server.cc:1088] HTTP: unable to provide 'OUTPUT__0' in GPU, will use CPU\" 는 에러가 아닙니다. 아래 내용 참조 하세요.\n",
    "    - this log comes from the HTTP / gRPC server. It's not an error. Since the output is returned via HTTP/GRPC the buffer resides on CPU not GPU even though your model outputs may have been on GPU.\n",
    "        - https://github.com/triton-inference-server/server/issues/2090\n",
    "\n",
    "![cloud_watch_food_log.png](img/cloud_watch_food_log.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.NCF Fashion 모델에 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "runtime_client = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "start_time = time.time()\n",
    "result = multiple_model_invoke_endpoint(runtime_client,endpoint_name, payload, fashion_model_tar_file)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('result: ', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 앤드포인트 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_utils import delete_endpoint\n",
    "\n",
    "client = boto3.Session().client('sagemaker')\n",
    "delete_endpoint(client, real_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/triton-inference-server/server/issues/2090\n",
    "\n",
    "this log comes from the HTTP / gRPC server. It's not an error. Since the output is returned via HTTP/GRPC the buffer resides on CPU not GPU even though your model outputs may have been on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
