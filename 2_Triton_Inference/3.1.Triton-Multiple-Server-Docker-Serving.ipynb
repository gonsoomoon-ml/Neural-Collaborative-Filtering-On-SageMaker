{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [모듈 3.1] Triton Docker 에 두 개의 NCF 모델 서빙"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 환경 셋업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1. 기본 세팅\n",
    "사용하는 패키지는 import 시점에 다시 재로딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('./src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. 배포 준비\n",
    "\n",
    "### 이전 노트북에서 훈련된 모델의 S3 경로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r artifact_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model artifact is assigend from :  s3://sagemaker-us-east-1-057716757052/pytorch-training-2022-12-11-07-36-01-118/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(\"model artifact is assigend from : \", artifact_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추론을 위한  데이터 세트 로딩\n",
    "- 전부 데이터를 로딩할 필요가 없지만, 여기서는 기존에 사용한 함수를 이용하기 위해서 전체 데이터를 로드 합니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_utils \n",
    "train_data, test_data, user_num ,item_num, train_mat = data_utils.load_all(test_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파라미터 생성\n",
    "- 모델 로딩시에 아라 파라미터 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batch_size:  256\n"
     ]
    }
   ],
   "source": [
    "class Params:\n",
    "    def __init__(self):\n",
    "        # self.epochs = 1        \n",
    "        self.num_ng = 4\n",
    "        self.batch_size = 256\n",
    "        self.test_num_ng = 99\n",
    "        self.factor_num = 32\n",
    "        self.num_layers = 3\n",
    "        self.dropout = 0.0\n",
    "        # self.lr = 0.001\n",
    "        self.top_k = 10\n",
    "        self.out = True\n",
    "        # self.gpu = \"0\"\n",
    "                        \n",
    "args = Params()\n",
    "print(\"# of batch_size: \", args.batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 훈련된 모델 아티펙트 다운로드 및 압축해제\n",
    "- 모델 아티펙트를 다운로드 합니다.\n",
    "- 다운로드 받은 모델 아티펙트의 압축을 해제하고 모델 가중치인 models/model.pth 파일을 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_data_dir:  ./models/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import config\n",
    "\n",
    "model_data_dir = config.model_path\n",
    "os.makedirs(model_data_dir, exist_ok=True)\n",
    "print(\"model_data_dir: \", model_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-057716757052/pytorch-training-2022-12-11-07-36-01-118/output/model.tar.gz\n",
      "./models/\n",
      "download: s3://sagemaker-us-east-1-057716757052/pytorch-training-2022-12-11-07-36-01-118/output/model.tar.gz to models/model.tar.gz\n",
      "NeuMF-end.pth\n"
     ]
    }
   ],
   "source": [
    "%%sh -s {artifact_path} {model_data_dir}\n",
    "\n",
    "artifact_path=$1\n",
    "model_data_dir=$2\n",
    "\n",
    "echo $artifact_path\n",
    "echo $model_data_dir\n",
    "\n",
    "# 기존 데이터 삭제\n",
    "rm -rf $model_data_dir/*\n",
    "\n",
    "# 모델을 S3에서 로컬로 다운로드\n",
    "aws s3 cp $artifact_path $model_data_dir\n",
    "\n",
    "# 모델 다운로드 폴더로 이동\n",
    "cd $model_data_dir\n",
    "\n",
    "# 압축 해제\n",
    "tar -xvf model.tar.gz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 훈련된 모델 로딩\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. 모델 네트워크 설정 저장\n",
    "- 모델 네트워크를 생성시에 설정값을 model_config.json 로 저장함.\n",
    "- model_fn() 함수에서 모델 네트워크를 생성시에 사용 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src/model_config.json is saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'src/model_config.json'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from common_utils import save_json, load_json\n",
    "\n",
    "model_config_dict = {\n",
    "    'user_num': str(user_num),\n",
    "    'item_num': str(item_num),\n",
    "    'factor_num' : str(args.factor_num),\n",
    "    'num_layers' : str(args.num_layers),\n",
    "    'dropout' : str(args.dropout),\n",
    "    'model_type': config.model\n",
    "}\n",
    "\n",
    "model_config_file = 'model_config.json'\n",
    "model_config_file_path = os.path.join('src', model_config_file)\n",
    "\n",
    "save_json(model_config_file_path, model_config_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.2. 두 개의 모델 생성\n",
    "- 복수개의 모델로 진행하기 위해서, 편의상 동일한 모델에서 생성 함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Staring model_fn() ###############\n",
      "--> model_dir : ./models/\n",
      "model_config_path: :  /home/ec2-user/SageMaker/Neural-Collaborative-Filtering-On-SageMaker/2_Triton_Inference/./src/model_config.json\n",
      "--> model network is loaded\n",
      "model_file_path: :  {model_file_path}\n",
      "####### Model is loaded #########\n",
      "######## Staring model_fn() ###############\n",
      "--> model_dir : ./models/\n",
      "model_config_path: :  /home/ec2-user/SageMaker/Neural-Collaborative-Filtering-On-SageMaker/2_Triton_Inference/./src/model_config.json\n",
      "--> model network is loaded\n",
      "model_file_path: :  {model_file_path}\n",
      "####### Model is loaded #########\n"
     ]
    }
   ],
   "source": [
    "from inference import model_fn\n",
    "\n",
    "ncf_food_model = model_fn(config.model_path)\n",
    "ncf_fashion_model = model_fn(config.model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NCF(\n",
       "  (embed_user_GMF): Embedding(6040, 32)\n",
       "  (embed_item_GMF): Embedding(3706, 32)\n",
       "  (embed_user_MLP): Embedding(6040, 128)\n",
       "  (embed_item_MLP): Embedding(3706, 128)\n",
       "  (linear): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (MLP_layers): Sequential(\n",
       "    (0): Dropout(p=0.0, inplace=False)\n",
       "    (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.0, inplace=False)\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Dropout(p=0.0, inplace=False)\n",
       "    (7): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (8): ReLU()\n",
       "  )\n",
       "  (predict_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncf_food_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Trition 서빙 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. 샘플 입력 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "dummy_inputs: \n",
      " [tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]], device='cuda:0', dtype=torch.int32), tensor([[478, 360, 373, 466, 131, 140, 524, 728, 400, 651, 371,  73, 904, 140,\n",
      "         991, 213, 459, 802,  15, 100, 758, 867, 644, 243, 920, 586,  29, 747,\n",
      "         247, 142, 710, 905, 425, 598, 478,  25, 139, 148, 229, 953, 100, 324,\n",
      "         732,  80, 323, 959, 473, 372,  81, 467, 290, 934, 500,  65, 832, 132,\n",
      "         144, 380, 471, 609,  34, 779,  99, 800,  40, 258, 781, 331, 530, 987,\n",
      "         978, 331, 401, 520, 922, 891, 915, 100, 352, 554, 726, 821, 864, 316,\n",
      "         310, 504,  90, 633, 614,  39, 876, 226,  28, 614, 137, 838, 200, 316,\n",
      "         829, 474]], device='cuda:0', dtype=torch.int32)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "user_np = np.zeros((1,100)).astype(np.int32)\n",
    "item_np = np.random.randint(low=1, high=1000, size=(1,100)).astype(np.int32)\n",
    "\n",
    "\n",
    "dummy_inputs = [\n",
    "    torch.from_numpy(user_np).to(device),\n",
    "    torch.from_numpy(item_np).to(device)\n",
    "]\n",
    "print(\"dummy_inputs: \\n\", dummy_inputs)\n",
    "dummy_user = dummy_inputs[0] \n",
    "dummy_item = dummy_inputs[1] \n",
    "\n",
    "# dummy_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. 샘플 입력으로 모델 추론 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result shape:  torch.Size([1, 100, 1])\n",
      "result shape:  torch.Size([1, 100, 1])\n"
     ]
    }
   ],
   "source": [
    "result = ncf_food_model(dummy_user, dummy_item)\n",
    "print(\"result shape: \", result.shape)\n",
    "result = ncf_fashion_model(dummy_user, dummy_item)\n",
    "print(\"result shape: \", result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Torch Script 으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_trace = True\n",
    "is_script = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As trace : Model is saved ncf_food_model.pt\n",
      "#### Load Test ####\n",
      "def forward(self,\n",
      "    input: Tensor,\n",
      "    input0: Tensor) -> Tensor:\n",
      "  _0 = self.predict_layer\n",
      "  _1 = self.MLP_layers\n",
      "  _2 = self.embed_item_MLP\n",
      "  _3 = self.embed_user_MLP\n",
      "  _4 = self.embed_item_GMF\n",
      "  _5 = (self.embed_user_GMF).forward(input, )\n",
      "  output_GMF = torch.mul(_5, (_4).forward(input0, ))\n",
      "  _6 = [(_3).forward(input, ), (_2).forward(input0, )]\n",
      "  input1 = torch.cat(_6, -1)\n",
      "  input2 = torch.cat([output_GMF, (_1).forward(input1, )], -1)\n",
      "  return (_0).forward(input2, )\n",
      "\n",
      "Result shape:  torch.Size([1, 100, 1])\n",
      "As trace : Model is saved ncf_fashion_model.pt\n",
      "#### Load Test ####\n",
      "def forward(self,\n",
      "    input: Tensor,\n",
      "    input0: Tensor) -> Tensor:\n",
      "  _0 = self.predict_layer\n",
      "  _1 = self.MLP_layers\n",
      "  _2 = self.embed_item_MLP\n",
      "  _3 = self.embed_user_MLP\n",
      "  _4 = self.embed_item_GMF\n",
      "  _5 = (self.embed_user_GMF).forward(input, )\n",
      "  output_GMF = torch.mul(_5, (_4).forward(input0, ))\n",
      "  _6 = [(_3).forward(input, ), (_2).forward(input0, )]\n",
      "  input1 = torch.cat(_6, -1)\n",
      "  input2 = torch.cat([output_GMF, (_1).forward(input1, )], -1)\n",
      "  return (_0).forward(input2, )\n",
      "\n",
      "Result shape:  torch.Size([1, 100, 1])\n"
     ]
    }
   ],
   "source": [
    "def trace_model(mode, device, model, dummy_inputs, trace_model_name):\n",
    "\n",
    "    model = model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    if mode == 'trace' :\n",
    "        IR_model = torch.jit.trace(model, dummy_inputs)\n",
    "\n",
    "    elif mode == 'script':\n",
    "        IR_model = torch.jit.script(model)\n",
    "\n",
    "    print(f\"As {mode} : Model is saved {trace_model_name}\")\n",
    "    torch.jit.save(IR_model, trace_model_name)\n",
    "\n",
    "    print(\"#### Load Test ####\")    \n",
    "    loaded_m = torch.jit.load(trace_model_name)    \n",
    "    print(loaded_m.code)    \n",
    "    dummy_user = dummy_inputs[0]\n",
    "    dummy_item = dummy_inputs[1]    \n",
    "    \n",
    "    result = loaded_m(dummy_user, dummy_item)\n",
    "    print(\"Result shape: \", result.shape)\n",
    "\n",
    "    \n",
    "    \n",
    "if is_trace:\n",
    "    mode = 'trace'    \n",
    "elif is_script:    \n",
    "    mode = 'script'\n",
    "\n",
    "# food\n",
    "trace_food_model_name = 'ncf_food_model.pt'    \n",
    "trace_model(mode, device, ncf_food_model, dummy_inputs, trace_food_model_name)    \n",
    "# fashion\n",
    "trace_fashion_model_name = 'ncf_fashion_model.pt'    \n",
    "trace_model(mode, device, ncf_fashion_model, dummy_inputs, trace_fashion_model_name)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.config.pbtxt 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ncf_food_config 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ncf_food_config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ncf_food_config.pbtxt\n",
    "name: \"ncf_food_model\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 128\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [100]\n",
    "  },\n",
    "  {\n",
    "    name: \"INPUT__1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [100]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [-1]\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ncf_fashion_config 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ncf_fashion_config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ncf_fashion_config.pbtxt\n",
    "name: \"ncf_fashion_model\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 128\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [100]\n",
    "  },\n",
    "  {\n",
    "    name: \"INPUT__1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [100]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [-1]\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 아티펙트 패키징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 복수 모델\n",
    "```\n",
    "model_serving_folder\n",
    "    - model_name\n",
    "        - version_number\n",
    "            - model file\n",
    "        - config file\n",
    "    - model_name\n",
    "        - version_number\n",
    "            - model file\n",
    "        - config file\n",
    "        \n",
    "triton-serve-pt\n",
    "    - ncf_food_model\n",
    "        - 1\n",
    "            - model.pt\n",
    "        - config.pbtxt\n",
    "    - ncf_fashion_model\n",
    "        - 1\n",
    "            - model.pt\n",
    "        - config.pbtxt\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. ncf_food_model 폴더 생성 및 아티펙트 카피"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from triton_util import make_folder_structure, copy_artifact, remove_folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triton-multi-docker-serve-pt:\n",
      "ncf_fashion_model\n",
      "ncf_food_model\n",
      "\n",
      "triton-multi-docker-serve-pt/ncf_fashion_model:\n",
      "1\n",
      "config.pbtxt\n",
      "\n",
      "triton-multi-docker-serve-pt/ncf_fashion_model/1:\n",
      "model.pt\n",
      "\n",
      "triton-multi-docker-serve-pt/ncf_food_model:\n",
      "1\n",
      "config.pbtxt\n",
      "\n",
      "triton-multi-docker-serve-pt/ncf_food_model/1:\n",
      "model.pt\n"
     ]
    }
   ],
   "source": [
    "# ncf_food_model 폴더 생성\n",
    "model_serving_folder = 'triton-multi-docker-serve-pt'\n",
    "food_model_name = 'ncf_food_model'\n",
    "make_folder_structure(model_serving_folder, food_model_name)\n",
    "\n",
    "fodd_config = 'ncf_food_config.pbtxt'\n",
    "copy_artifact(model_serving_folder, food_model_name, trace_food_model_name, fodd_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. ncf_fashion_model 폴더 생성 및 아티펙트 카피"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triton-multi-docker-serve-pt:\n",
      "ncf_fashion_model\n",
      "ncf_food_model\n",
      "\n",
      "triton-multi-docker-serve-pt/ncf_fashion_model:\n",
      "1\n",
      "config.pbtxt\n",
      "\n",
      "triton-multi-docker-serve-pt/ncf_fashion_model/1:\n",
      "model.pt\n",
      "\n",
      "triton-multi-docker-serve-pt/ncf_food_model:\n",
      "1\n",
      "config.pbtxt\n",
      "\n",
      "triton-multi-docker-serve-pt/ncf_food_model/1:\n",
      "model.pt\n"
     ]
    }
   ],
   "source": [
    "# ncf_food_model 폴더 생성\n",
    "fashion_model_name = 'ncf_fashion_model'\n",
    "make_folder_structure(model_serving_folder, fashion_model_name)\n",
    "\n",
    "fashion_config = 'ncf_fashion_config.pbtxt'\n",
    "copy_artifact(model_serving_folder, fashion_model_name, trace_fashion_model_name, fashion_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 폴더 삭제\n",
    "- 필요시 주석 제거하고 사용하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_serving_folder = 'triton-serve-pt'\n",
    "# remove_folder(model_serving_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. 로컬 도커에서 실행 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from triton_util import setup_triton_client\n",
    "triton_client, grpcclient = setup_triton_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_client_payload():\n",
    "    inputs = []\n",
    "\n",
    "    inputs.append(grpcclient.InferInput('INPUT__0', [1,100], \"INT32\"))\n",
    "    inputs.append(grpcclient.InferInput('INPUT__1', [1,100], \"INT32\"))\n",
    "\n",
    "    # user\n",
    "    input0_data = np.zeros((1,100)).astype(np.int32)\n",
    "    inputs[0].set_data_from_numpy(input0_data)\n",
    "\n",
    "    # item\n",
    "    input1_data = np.random.randint(low=1, high=1000, size=(1,100)).astype(np.int32)\n",
    "    inputs[1].set_data_from_numpy(input1_data)\n",
    "\n",
    "    print(\"input0_data: \\n\",input0_data) \n",
    "    print(\"input1_data: \\n\",input1_data) \n",
    "    \n",
    "    return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input0_data: \n",
      " [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "input1_data: \n",
      " [[193 708 780 727 630 962 372 664 892 431 848 824 870 341 448   6 742 795\n",
      "  293 815 586 708 605 852 985 441 702 744 406 581 386 866 184   9 337 368\n",
      "  115 559 918 478 922 825 798 505 785 511 144 186 134 711 689 571 208 304\n",
      "  368 319 457 788 403 625 770 258 486 737 523 133 841  28  54  63 729 229\n",
      "   97  89 196 485 493 463 785 353 764 701 326 769 603 194 125 396 316 559\n",
      "  776 950 241 895  97 729 224 570  68 429]]\n"
     ]
    }
   ],
   "source": [
    "inputs = create_client_payload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "outputs.append(grpcclient.InferRequestedOutput('OUTPUT__0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def infer_triton_client(model_name, inputs, outputs):\n",
    "    # Test with outputs\n",
    "    results = triton_client.infer(model_name=model_name,\n",
    "                                    inputs=inputs,\n",
    "                                    outputs=outputs,\n",
    "                                    headers={'test': '1'})\n",
    "\n",
    "    # Get the output arrays from the results\n",
    "    output0_data = results.as_numpy('OUTPUT__0')\n",
    "    print(\"#### output #####\")\n",
    "    print(output0_data.shape)\n",
    "    \n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### output #####\n",
      "(1, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"ncf_food_model\"\n",
    "infer_triton_client(food_model_name, inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### output #####\n",
      "(1, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"ncf_fashion_model\"\n",
    "infer_triton_client(fashion_model_name, inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0. 도커에서의 실행 테스트는 아래와 같은 순서로 진행 함.\n",
    "\n",
    "#### (0) Triton Client 초기화\n",
    "```\n",
    "from triton_util import setup_triton_client\n",
    "triton_client, grpcclient = setup_triton_client()\n",
    "```\n",
    "\n",
    "#### (1) 터미널 실행\n",
    "![terminal.png](img/terminal.png)\n",
    "\n",
    "#### (2) Triton 도커 컨테이너 실행\n",
    "- 위의 터미널에 아래와 같이 명령어를 하나씩 실행 하세요.\n",
    "```\n",
    "cd /home/ec2-user/SageMaker/Neural-Collaborative-Filtering-On-SageMaker/2_Triton_Inference\n",
    "\n",
    "docker run --gpus=1 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v `pwd`/triton-multi-docker-serve-pt:/models nvcr.io/nvidia/tritonserver:22.08-py3 tritonserver --model-repository=/models --log-verbose=3 --log-info=1 --log-warning=1 --log-error=1\n",
    "```\n",
    "#### (3) Triton 클라이언트로 추론 실행\n",
    "#### (4) 도커 중단 및 삭제\n",
    "```\n",
    "docker rm -f $(docker ps -qa)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Triton Client 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from triton_util import setup_triton_client\n",
    "triton_client, grpcclient = setup_triton_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. !!! #### 터미널에 \"Triton 도커 컨테이너 실행\" 을 해주세요. ### !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. 입력 payload 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_client_payload():\n",
    "    inputs = []\n",
    "\n",
    "    inputs.append(grpcclient.InferInput('INPUT__0', [1,100], \"INT32\"))\n",
    "    inputs.append(grpcclient.InferInput('INPUT__1', [1,100], \"INT32\"))\n",
    "\n",
    "    # user\n",
    "    input0_data = np.zeros((1,100)).astype(np.int32)\n",
    "    inputs[0].set_data_from_numpy(input0_data)\n",
    "\n",
    "    # item\n",
    "    input1_data = np.random.randint(low=1, high=1000, size=(1,100)).astype(np.int32)\n",
    "    inputs[1].set_data_from_numpy(input1_data)\n",
    "\n",
    "    print(\"input0_data: \\n\",input0_data) \n",
    "    print(\"input1_data: \\n\",input1_data) \n",
    "    \n",
    "    return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input0_data: \n",
      " [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "input1_data: \n",
      " [[109 627 929 672 292 694 498 605 741 222 857 476 475 144 826 537 188 615\n",
      "  619 105 735 618 320 205 689 324 123  46 675 279 457 718 270 126  34 277\n",
      "  867  96 534 784 215 648 321 303 973 482 591 901 341 519  85 681 670 480\n",
      "  762 692 896 803 902 269 337 349 964 922 563 864 636 584 567  82 729 609\n",
      "  853 479 239 196 218 661 501  72 406 192 656 287 825  60 622 790  61 283\n",
      "  839 563 430 885 157  99 947 617 643 358]]\n"
     ]
    }
   ],
   "source": [
    "inputs = create_client_payload()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. 출력 변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "outputs.append(grpcclient.InferRequestedOutput('OUTPUT__0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6.5. Triton에 추론 요청\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### output #####\n",
      "(1, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "from triton_util import infer_triton_client\n",
    "\n",
    "infer_triton_client(triton_client, food_model_name, inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### output #####\n",
      "(1, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "infer_triton_client(triton_client, fashion_model_name, inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. 변수 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'model_serving_folder' (str)\n",
      "Stored 'food_model_name' (str)\n",
      "Stored 'fashion_model_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store model_serving_folder\n",
    "%store food_model_name\n",
    "%store fashion_model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
