{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Module 1.2] 로컬 모드 및 스크립트 모드로 훈련 (SageMaker 사용)\n",
    "\n",
    "### 본 워크샵의 모든 노트북은 `conda_python3` 여기에서 작업 합니다.\n",
    "\n",
    "이 노트북은 아래와 같은 작업을 합니다.\n",
    "- 1. 환경 셋업\n",
    "- 2. 세이지 메이크 로컬 모드 훈련\n",
    "- 3. SageMaker Host Mode 로 훈련\n",
    "- 4. 모델 아티펙트 저장\n",
    "\n",
    "---\n",
    "\n",
    "참고:\n",
    "\n",
    "- 세이지 메이커로 파이토치 사용 \n",
    "    - [Use PyTorch with the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html)\n",
    "\n",
    "\n",
    "- Use PyTorch with the SageMaker Python SDK\n",
    "    - https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html\n",
    "\n",
    "\n",
    "- Amazon SageMaker Local Mode Examples\n",
    "    - TF, Pytorch, SKLean, SKLearn Processing JOb에 대한 로컬 모드 샘플\n",
    "        - https://github.com/aws-samples/amazon-sagemaker-local-mode\n",
    "    - Pytorch 로컬 모드\n",
    "        - https://github.com/aws-samples/amazon-sagemaker-local-mode/blob/main/pytorch_script_mode_local_training_and_serving/pytorch_script_mode_local_training_and_serving.py    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 환경 셋업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 세팅\n",
    "사용하는 패키지는 import 시점에 다시 재로딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('./src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 커스텀 라이브러리\n",
    "import config "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "버킷 및 폴더(prefix) 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "use_default_bucket = True\n",
    "if use_default_bucket:\n",
    "    bucket = sagemaker_session.default_bucket()\n",
    "else:\n",
    "    bucket = '<Type your bucket>'\n",
    "    \n",
    "prefix = \"NCFModel\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 세이지 메이크 로컬 모드 훈련\n",
    "#### 로컬의 GPU, CPU 여부로 instance_type 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec 11 07:32:15 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:1B.0 Off |                    0 |\n",
      "| N/A   42C    P0    40W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:00:1C.0 Off |                    0 |\n",
      "| N/A   38C    P0    37W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:00:1D.0 Off |                    0 |\n",
      "| N/A   39C    P0    42W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   42C    P0    43W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Instance type = local_gpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "try:\n",
    "    if subprocess.call(\"nvidia-smi\") == 0:\n",
    "        ## Set type to GPU if one is present\n",
    "        instance_type = \"local_gpu\"\n",
    "    else:\n",
    "        instance_type = \"local\"        \n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Instance type = \" + instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. 스크립트 모드의 코드 작성 방법\n",
    "- ![script_mode_example](img/script_mode_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.훈련 코드 확인\n",
    "- 아래의 코드는 전형적인 스크립트 모드의 코드 작성 방법을 따르고 있습니다.\n",
    "- 훈련 함수는 `from train_lib import train` 로서 이전 노트북의 **[세이지 메이커 없이]** 작성한 스크래치 버전에서 사용한 훈련 함수와 동일 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_code = 'src/train.py'\n",
    "# !pygmentize {train_code}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. 로컬에 있는 데이타 세트의 위치를 지정 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_inputs:  ../data/\n"
     ]
    }
   ],
   "source": [
    "local_inputs = config.main_path\n",
    "print(\"local_inputs: \", local_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_inputs = {'train': f'file://{local_inputs}',\n",
    "          'test': f'file://{local_inputs}'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. 로컬 모드로 훈련 실행\n",
    "- 아래의 두 라인이 로컬모드로 훈련을 지시 합니다.\n",
    "```python\n",
    "    instance_type=instance_type, # local_gpu or local 지정\n",
    "    session = sagemaker.LocalSession(), # 로컬 세션을 사용합니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'epochs': 1, \n",
    "                   'lr': 0.001,\n",
    "                   'batch_size': 256,\n",
    "                   'top_k' : 10,\n",
    "                   'dropout' : 0.0,\n",
    "                   'factor_num' : 32,\n",
    "                   'num_layers' : 3,\n",
    "                   'num_ng' : 4,\n",
    "                   'test_num_ng' : 99,                   \n",
    "                    }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Creating quqz376y1p-algo-1-moq1f ... \n",
      "Creating quqz376y1p-algo-1-moq1f ... done\n",
      "Attaching to quqz376y1p-algo-1-moq1f\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m 2022-12-11 07:32:26,688 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m 2022-12-11 07:32:26,734 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m 2022-12-11 07:32:26,754 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m 2022-12-11 07:32:27,545 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m \n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m Training Env:\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m \n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m {\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m         \"train\": \"/opt/ml/input/data/train\",\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m         \"test\": \"/opt/ml/input/data/test\"\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     },\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     \"current_host\": \"algo-1-moq1f\",\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     \"hosts\": [\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m         \"algo-1-moq1f\"\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     ],\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m         \"epochs\": 1,\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m         \"lr\": 0.001,\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m         \"batch_size\": 256,\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m         \"top_k\": 10,\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m         \"dropout\": 0.0,\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m         \"factor_num\": 32,\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m         \"num_layers\": 3,\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m         \"num_ng\": 4,\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m         \"test_num_ng\": 99\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     },\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m         \"train\": {\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m         },\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m         \"test\": {\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m         }\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     },\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     \"job_name\": \"pytorch-training-2022-12-11-07-32-16-046\",\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     \"master_hostname\": \"algo-1-moq1f\",\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     \"module_dir\": \"s3://sagemaker-us-east-1-057716757052/pytorch-training-2022-12-11-07-32-16-046/source/sourcedir.tar.gz\",\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     \"module_name\": \"train\",\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     \"num_cpus\": 32,\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     \"num_gpus\": 4,\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m         \"current_host\": \"algo-1-moq1f\",\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m         \"hosts\": [\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m             \"algo-1-moq1f\"\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m         ]\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     },\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m     \"user_entry_point\": \"train.py\"\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m }\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m \n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m Environment variables:\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m \n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_HOSTS=[\"algo-1-moq1f\"]\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_HPS={\"batch_size\":256,\"dropout\":0.0,\"epochs\":1,\"factor_num\":32,\"lr\":0.001,\"num_layers\":3,\"num_ng\":4,\"test_num_ng\":99,\"top_k\":10}\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_USER_ENTRY_POINT=train.py\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-moq1f\",\"hosts\":[\"algo-1-moq1f\"]}\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_INPUT_DATA_CONFIG={\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_CHANNELS=[\"test\",\"train\"]\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_CURRENT_HOST=algo-1-moq1f\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_MODULE_NAME=train\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_NUM_CPUS=32\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_NUM_GPUS=4\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_MODULE_DIR=s3://sagemaker-us-east-1-057716757052/pytorch-training-2022-12-11-07-32-16-046/source/sourcedir.tar.gz\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1-moq1f\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-moq1f\"],\"hyperparameters\":{\"batch_size\":256,\"dropout\":0.0,\"epochs\":1,\"factor_num\":32,\"lr\":0.001,\"num_layers\":3,\"num_ng\":4,\"test_num_ng\":99,\"top_k\":10},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-12-11-07-32-16-046\",\"log_level\":20,\"master_hostname\":\"algo-1-moq1f\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-057716757052/pytorch-training-2022-12-11-07-32-16-046/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-moq1f\",\"hosts\":[\"algo-1-moq1f\"]},\"user_entry_point\":\"train.py\"}\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_USER_ARGS=[\"--batch_size\",\"256\",\"--dropout\",\"0.0\",\"--epochs\",\"1\",\"--factor_num\",\"32\",\"--lr\",\"0.001\",\"--num_layers\",\"3\",\"--num_ng\",\"4\",\"--test_num_ng\",\"99\",\"--top_k\",\"10\"]\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_CHANNEL_TEST=/opt/ml/input/data/test\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_HP_EPOCHS=1\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_HP_LR=0.001\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_HP_BATCH_SIZE=256\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_HP_TOP_K=10\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_HP_DROPOUT=0.0\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_HP_FACTOR_NUM=32\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_HP_NUM_LAYERS=3\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_HP_NUM_NG=4\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m SM_HP_TEST_NUM_NG=99\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m \n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m \n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m /opt/conda/bin/python3.6 train.py --batch_size 256 --dropout 0.0 --epochs 1 --factor_num 32 --lr 0.001 --num_layers 3 --num_ng 4 --test_num_ng 99 --top_k 10\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m \n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m \n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m ##### Args: \n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m  Namespace(batch_size=256, dropout=0.0, epochs=1, factor_num=32, gpu='0', lr=0.001, model_dir='/opt/ml/model', num_layers=3, num_ng=4, out=True, test_data_dir='/opt/ml/input/data/test', test_num_ng=99, top_k=10, train_data_dir='/opt/ml/input/data/train')\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m args.train_data_dir: \n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m args.test_data_dir: \n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m args.model_dir: \n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m =====> data loading <===========\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m Get train data sampler and data loader\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m Get test data sampler and data loader\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m Pretrained model is NOT used\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m labels_ps:  994169\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m labels_ng:  3976676\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m total train size :  4970845\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m =====> Starting New Traiing <===========\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m [2022-12-11 07:33:30.221 algo-1-moq1f:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m [2022-12-11 07:33:30.357 algo-1-moq1f:26 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m Train Epoch: 0 [256000/4970845 (5%)] Loss=0.319731;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m Train Epoch: 0 [512000/4970845 (10%)] Loss=0.412011;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m Train Epoch: 0 [768000/4970845 (15%)] Loss=0.344923;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m Train Epoch: 0 [1024000/4970845 (21%)] Loss=0.348968;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m Train Epoch: 0 [1280000/4970845 (26%)] Loss=0.344377;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m Train Epoch: 0 [1536000/4970845 (31%)] Loss=0.315511;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m Train Epoch: 0 [1792000/4970845 (36%)] Loss=0.313027;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m Train Epoch: 0 [2048000/4970845 (41%)] Loss=0.330411;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m Train Epoch: 0 [2304000/4970845 (46%)] Loss=0.276141;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m Train Epoch: 0 [2560000/4970845 (51%)] Loss=0.253683;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m Train Epoch: 0 [2816000/4970845 (57%)] Loss=0.295026;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m Train Epoch: 0 [3072000/4970845 (62%)] Loss=0.293857;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m Train Epoch: 0 [3328000/4970845 (67%)] Loss=0.331499;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m Train Epoch: 0 [3584000/4970845 (72%)] Loss=0.264499;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m Train Epoch: 0 [3840000/4970845 (77%)] Loss=0.268829;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m Train Epoch: 0 [4096000/4970845 (82%)] Loss=0.255600;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m Train Epoch: 0 [4352000/4970845 (88%)] Loss=0.245861;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m Train Epoch: 0 [4608000/4970845 (93%)] Loss=0.303751;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m Train Epoch: 0 [4864000/4970845 (98%)] Loss=0.269486;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m The time elapse of epoch 000 is: 00: 02: 06\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m cuda\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m HR=0.632; \t NDCG=0.360;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m best_hr:  0.631953642384106\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m the model is saved at /opt/ml/model/NeuMF-end.pth\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m End. Best epoch 000: HR = 0.632, NDCG = 0.360\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m INFO:train_lib:Train Epoch: 0 [256000/4970845 (5%)] Loss=0.319731;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m INFO:train_lib:Train Epoch: 0 [512000/4970845 (10%)] Loss=0.412011;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m INFO:train_lib:Train Epoch: 0 [768000/4970845 (15%)] Loss=0.344923;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m INFO:train_lib:Train Epoch: 0 [1024000/4970845 (21%)] Loss=0.348968;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m INFO:train_lib:Train Epoch: 0 [1280000/4970845 (26%)] Loss=0.344377;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m INFO:train_lib:Train Epoch: 0 [1536000/4970845 (31%)] Loss=0.315511;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m INFO:train_lib:Train Epoch: 0 [1792000/4970845 (36%)] Loss=0.313027;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m INFO:train_lib:Train Epoch: 0 [2048000/4970845 (41%)] Loss=0.330411;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m INFO:train_lib:Train Epoch: 0 [2304000/4970845 (46%)] Loss=0.276141;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m INFO:train_lib:Train Epoch: 0 [2560000/4970845 (51%)] Loss=0.253683;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m INFO:train_lib:Train Epoch: 0 [2816000/4970845 (57%)] Loss=0.295026;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m INFO:train_lib:Train Epoch: 0 [3072000/4970845 (62%)] Loss=0.293857;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m INFO:train_lib:Train Epoch: 0 [3328000/4970845 (67%)] Loss=0.331499;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m INFO:train_lib:Train Epoch: 0 [3584000/4970845 (72%)] Loss=0.264499;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m INFO:train_lib:Train Epoch: 0 [3840000/4970845 (77%)] Loss=0.268829;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m INFO:train_lib:Train Epoch: 0 [4096000/4970845 (82%)] Loss=0.255600;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m INFO:train_lib:Train Epoch: 0 [4352000/4970845 (88%)] Loss=0.245861;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m INFO:train_lib:Train Epoch: 0 [4608000/4970845 (93%)] Loss=0.303751;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m INFO:train_lib:Train Epoch: 0 [4864000/4970845 (98%)] Loss=0.269486;\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m \n",
      "\u001b[36mquqz376y1p-algo-1-moq1f |\u001b[0m 2022-12-11 07:35:58,687 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\u001b[36mquqz376y1p-algo-1-moq1f exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "local_estimator = PyTorch(\n",
    "    entry_point=\"train.py\",    \n",
    "    source_dir='src',    \n",
    "    role=role,\n",
    "    framework_version='1.8.1',\n",
    "    py_version='py3',\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type, # local_gpu or local 지정\n",
    "    session = sagemaker.LocalSession(), # 로컬 세션을 사용합니다.\n",
    "    hyperparameters= hyperparameters               \n",
    "    \n",
    ")\n",
    "local_estimator.fit(local_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SageMaker Host Mode 로 훈련\n",
    "- instance_type, session 을 수정 합니다.\n",
    "- 입력 데이터를 inputs로서 S3 의 경로를 제공합니다.\n",
    "- wait=False 로 지정해서 async 모드로 훈련을 실행합니다. \n",
    "- 실행 경과는 아래의 cifar10_estimator.logs() 에서 확인 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.1. 데이터 세트를 S3에 업로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_data_loc:  s3://sagemaker-us-east-1-057716757052/NCFModel/data\n"
     ]
    }
   ],
   "source": [
    "s3_data_loc = sagemaker_session.upload_data(path=config.main_path, bucket=bucket, \n",
    "                                       key_prefix=f\"{prefix}/data\")\n",
    "print(\"s3_data_loc: \", s3_data_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-11 07:36:00    2891424 NCFModel/data/ml-1m.test.negative\n",
      "2022-12-11 07:36:00     128039 NCFModel/data/ml-1m.test.rating\n",
      "2022-12-11 07:36:00   20982911 NCFModel/data/ml-1m.train.rating\n"
     ]
    }
   ],
   "source": [
    "! aws s3 ls {s3_data_loc} --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. 훈련 및 테스트 데이터를 S3 로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_inputs: \n",
      " {'train': 's3://sagemaker-us-east-1-057716757052/NCFModel/data', 'test': 's3://sagemaker-us-east-1-057716757052/NCFModel/data'}\n"
     ]
    }
   ],
   "source": [
    "s3_inputs = {\n",
    "            'train': f'{s3_data_loc}',\n",
    "            'test': f'{s3_data_loc}'\n",
    "            }\n",
    "\n",
    "print(\"s3_inputs: \\n\", s3_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하이퍼 파라미터 세팅\n",
    "- epochs 값을 조절해서 실행 시간을 조정 하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_hyperparameters = {'epochs': 1, \n",
    "                       'lr': 0.001,\n",
    "                       'batch_size': 256,\n",
    "                       'top_k' : 10,\n",
    "                       'dropout' : 0.0,\n",
    "                       'factor_num' : 32,\n",
    "                       'num_layers' : 3,\n",
    "                       'num_ng' : 4,\n",
    "                       'test_num_ng' : 99,                   \n",
    "                    }  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 훈련 실행\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 메트릭을 CloudWatch 에서 보기\n",
    "- 개발자 가이드\n",
    "    - [Monitor and Analyze Training Jobs Using Amazon CloudWatch ](https://docs.amazonaws.cn/en_us/sagemaker/latest/dg/training-metrics.html#define-train-metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions=[\n",
    "       {'Name': 'HR', 'Regex': 'HR=(.*?);'},\n",
    "       {'Name': 'NDCG', 'Regex': 'NDCG=(.*?);'},\n",
    "       {'Name': 'Loss', 'Regex': 'Loss=(.*?);'}        \n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "instance_type = 'ml.p3.2xlarge'\n",
    "\n",
    "host_estimator = PyTorch(\n",
    "    entry_point=\"train.py\",    \n",
    "    source_dir='src',    \n",
    "    role=role,\n",
    "    framework_version='1.8.1',\n",
    "    py_version='py3',\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    session = sagemaker.Session(), # 세이지 메이커 세션\n",
    "    hyperparameters=host_hyperparameters,\n",
    "    metric_definitions = metric_definitions\n",
    "    \n",
    ")\n",
    "host_estimator.fit(s3_inputs, \n",
    "                   # experiment_config = experiment_config, # 실험 설정 제공                   \n",
    "                   wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-11 07:36:01 Starting - Starting the training job...\n",
      "2022-12-11 07:36:26 Starting - Preparing the instances for trainingProfilerReport-1670744161: InProgress\n",
      ".........\n",
      "2022-12-11 07:37:52 Downloading - Downloading input data...\n",
      "2022-12-11 07:38:25 Training - Downloading the training image.....................\n",
      "2022-12-11 07:41:58 Training - Training image download completed. Training in progress...\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-12-11 07:42:15,757 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-12-11 07:42:15,786 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-12-11 07:42:15,789 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-12-11 07:42:16,083 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 256,\n",
      "        \"dropout\": 0.0,\n",
      "        \"epochs\": 1,\n",
      "        \"factor_num\": 32,\n",
      "        \"lr\": 0.001,\n",
      "        \"num_layers\": 3,\n",
      "        \"num_ng\": 4,\n",
      "        \"test_num_ng\": 99,\n",
      "        \"top_k\": 10\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2022-12-11-07-36-01-118\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-057716757052/pytorch-training-2022-12-11-07-36-01-118/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":256,\"dropout\":0.0,\"epochs\":1,\"factor_num\":32,\"lr\":0.001,\"num_layers\":3,\"num_ng\":4,\"test_num_ng\":99,\"top_k\":10}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-057716757052/pytorch-training-2022-12-11-07-36-01-118/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":256,\"dropout\":0.0,\"epochs\":1,\"factor_num\":32,\"lr\":0.001,\"num_layers\":3,\"num_ng\":4,\"test_num_ng\":99,\"top_k\":10},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-12-11-07-36-01-118\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-057716757052/pytorch-training-2022-12-11-07-36-01-118/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"256\",\"--dropout\",\"0.0\",\"--epochs\",\"1\",\"--factor_num\",\"32\",\"--lr\",\"0.001\",\"--num_layers\",\"3\",\"--num_ng\",\"4\",\"--test_num_ng\",\"99\",\"--top_k\",\"10\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=256\u001b[0m\n",
      "\u001b[34mSM_HP_DROPOUT=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_FACTOR_NUM=32\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.001\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_LAYERS=3\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_NG=4\u001b[0m\n",
      "\u001b[34mSM_HP_TEST_NUM_NG=99\u001b[0m\n",
      "\u001b[34mSM_HP_TOP_K=10\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train.py --batch_size 256 --dropout 0.0 --epochs 1 --factor_num 32 --lr 0.001 --num_layers 3 --num_ng 4 --test_num_ng 99 --top_k 10\u001b[0m\n",
      "\u001b[34m##### Args: \n",
      " Namespace(batch_size=256, dropout=0.0, epochs=1, factor_num=32, gpu='0', lr=0.001, model_dir='/opt/ml/model', num_layers=3, num_ng=4, out=True, test_data_dir='/opt/ml/input/data/test', test_num_ng=99, top_k=10, train_data_dir='/opt/ml/input/data/train')\u001b[0m\n",
      "\u001b[34margs.train_data_dir: \u001b[0m\n",
      "\u001b[34margs.test_data_dir: \u001b[0m\n",
      "\u001b[34margs.model_dir: \u001b[0m\n",
      "\u001b[34m=====> data loading <===========\u001b[0m\n",
      "\u001b[34mGet train data sampler and data loader\u001b[0m\n",
      "\u001b[34mGet test data sampler and data loader\u001b[0m\n",
      "\u001b[34mPretrained model is NOT used\u001b[0m\n",
      "\u001b[34mlabels_ps:  994169\u001b[0m\n",
      "\u001b[34mlabels_ng:  3976676\u001b[0m\n",
      "\u001b[34mtotal train size :  4970845\u001b[0m\n",
      "\u001b[34m=====> Starting New Traiing <===========\u001b[0m\n",
      "\u001b[34m[2022-12-11 07:43:14.960 algo-1:27 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-12-11 07:43:15.003 algo-1:27 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-12-11 07:43:15.003 algo-1:27 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-12-11 07:43:15.003 algo-1:27 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-12-11 07:43:15.004 algo-1:27 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-12-11 07:43:15.004 algo-1:27 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-12-11 07:43:16.190 algo-1:27 INFO hook.py:591] name:embed_user_GMF.weight count_params:193280\u001b[0m\n",
      "\u001b[34m[2022-12-11 07:43:16.190 algo-1:27 INFO hook.py:591] name:embed_item_GMF.weight count_params:118592\u001b[0m\n",
      "\u001b[34m[2022-12-11 07:43:16.190 algo-1:27 INFO hook.py:591] name:embed_user_MLP.weight count_params:773120\u001b[0m\n",
      "\u001b[34m[2022-12-11 07:43:16.190 algo-1:27 INFO hook.py:591] name:embed_item_MLP.weight count_params:474368\u001b[0m\n",
      "\u001b[34m[2022-12-11 07:43:16.191 algo-1:27 INFO hook.py:591] name:linear.weight count_params:16\u001b[0m\n",
      "\u001b[34m[2022-12-11 07:43:16.191 algo-1:27 INFO hook.py:591] name:linear.bias count_params:4\u001b[0m\n",
      "\u001b[34m[2022-12-11 07:43:16.191 algo-1:27 INFO hook.py:591] name:MLP_layers.1.weight count_params:32768\u001b[0m\n",
      "\u001b[34m[2022-12-11 07:43:16.191 algo-1:27 INFO hook.py:591] name:MLP_layers.1.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2022-12-11 07:43:16.191 algo-1:27 INFO hook.py:591] name:MLP_layers.4.weight count_params:8192\u001b[0m\n",
      "\u001b[34m[2022-12-11 07:43:16.191 algo-1:27 INFO hook.py:591] name:MLP_layers.4.bias count_params:64\u001b[0m\n",
      "\u001b[34m[2022-12-11 07:43:16.191 algo-1:27 INFO hook.py:591] name:MLP_layers.7.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[2022-12-11 07:43:16.191 algo-1:27 INFO hook.py:591] name:MLP_layers.7.bias count_params:32\u001b[0m\n",
      "\u001b[34m[2022-12-11 07:43:16.191 algo-1:27 INFO hook.py:591] name:predict_layer.weight count_params:64\u001b[0m\n",
      "\u001b[34m[2022-12-11 07:43:16.191 algo-1:27 INFO hook.py:591] name:predict_layer.bias count_params:1\u001b[0m\n",
      "\u001b[34m[2022-12-11 07:43:16.191 algo-1:27 INFO hook.py:593] Total Trainable Params: 1602677\u001b[0m\n",
      "\u001b[34m[2022-12-11 07:43:16.192 algo-1:27 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-12-11 07:43:16.195 algo-1:27 INFO hook.py:488] Hook is writing from the hook with pid: 27\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [256000/4970845 (5%)] Loss=0.398165;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [512000/4970845 (10%)] Loss=0.396550;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [768000/4970845 (15%)] Loss=0.371629;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [1024000/4970845 (21%)] Loss=0.358348;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [1280000/4970845 (26%)] Loss=0.363186;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [1536000/4970845 (31%)] Loss=0.377794;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [1792000/4970845 (36%)] Loss=0.285643;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [2048000/4970845 (41%)] Loss=0.356889;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [2304000/4970845 (46%)] Loss=0.282204;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [2560000/4970845 (51%)] Loss=0.245586;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [2816000/4970845 (57%)] Loss=0.342785;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [3072000/4970845 (62%)] Loss=0.293584;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [3328000/4970845 (67%)] Loss=0.257422;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [3584000/4970845 (72%)] Loss=0.246583;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [3840000/4970845 (77%)] Loss=0.265939;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [4096000/4970845 (82%)] Loss=0.293166;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [4352000/4970845 (88%)] Loss=0.267653;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [4608000/4970845 (93%)] Loss=0.356308;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [4864000/4970845 (98%)] Loss=0.233297;\u001b[0m\n",
      "\u001b[34mThe time elapse of epoch 000 is: 00: 02: 10\u001b[0m\n",
      "\u001b[34mcuda\u001b[0m\n",
      "\u001b[34mHR=0.627; #011 NDCG=0.362;\u001b[0m\n",
      "\u001b[34mbest_hr:  0.6266556291390728\u001b[0m\n",
      "\u001b[34mthe model is saved at /opt/ml/model/NeuMF-end.pth\u001b[0m\n",
      "\u001b[34mEnd. Best epoch 000: HR = 0.627, NDCG = 0.362\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [256000/4970845 (5%)] Loss=0.398165;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [512000/4970845 (10%)] Loss=0.396550;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [768000/4970845 (15%)] Loss=0.371629;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [1024000/4970845 (21%)] Loss=0.358348;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [1280000/4970845 (26%)] Loss=0.363186;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [1536000/4970845 (31%)] Loss=0.377794;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [1792000/4970845 (36%)] Loss=0.285643;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [2048000/4970845 (41%)] Loss=0.356889;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [2304000/4970845 (46%)] Loss=0.282204;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [2560000/4970845 (51%)] Loss=0.245586;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [2816000/4970845 (57%)] Loss=0.342785;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [3072000/4970845 (62%)] Loss=0.293584;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [3328000/4970845 (67%)] Loss=0.257422;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [3584000/4970845 (72%)] Loss=0.246583;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [3840000/4970845 (77%)] Loss=0.265939;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [4096000/4970845 (82%)] Loss=0.293166;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [4352000/4970845 (88%)] Loss=0.267653;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [4608000/4970845 (93%)] Loss=0.356308;\u001b[0m\n",
      "\u001b[34m2022-12-11 07:45:46,848 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [4864000/4970845 (98%)] Loss=0.233297;\u001b[0m\n",
      "\n",
      "2022-12-11 07:46:07 Uploading - Uploading generated training model\n",
      "2022-12-11 07:46:07 Completed - Training job completed\n",
      "ProfilerReport-1670744161: NoIssuesFound\n",
      "Training seconds: 494\n",
      "Billable seconds: 494\n",
      "CPU times: user 1.07 s, sys: 101 ms, total: 1.18 s\n",
      "Wall time: 10min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "host_estimator.logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 모델 아티펙트 저장\n",
    "- S3 에 저장된 모델 아티펙트를 저장하여 추론시 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifact_path:  s3://sagemaker-us-east-1-057716757052/pytorch-training-2022-12-11-07-36-01-118/output/model.tar.gz\n",
      "Stored 'artifact_path' (str)\n"
     ]
    }
   ],
   "source": [
    "artifact_path = host_estimator.model_data\n",
    "print(\"artifact_path: \", artifact_path)\n",
    "\n",
    "%store artifact_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기타 변수 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %store prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
